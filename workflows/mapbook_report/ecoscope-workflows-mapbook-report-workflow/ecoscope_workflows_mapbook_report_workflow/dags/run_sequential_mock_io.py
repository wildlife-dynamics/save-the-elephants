# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection as set_gee_connection
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.transformation import (
    filter_row_values as filter_row_values,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps as set_base_maps
from ecoscope_workflows_ext_ste.tasks import (
    annotate_gdf_dict_with_geom_type as annotate_gdf_dict_with_geom_type,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_layers_from_gdf_dict as create_layers_from_gdf_dict,
)
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import get_file_path as get_file_path
from ecoscope_workflows_ext_ste.tasks import make_text_layer as make_text_layer
from ecoscope_workflows_ext_ste.tasks import split_gdf_by_column as split_gdf_by_column

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_polyline_layer as create_polyline_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap as draw_ecomap
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    classify_is_night as classify_is_night,
)
from ecoscope_workflows_ext_ste.tasks import combine_map_layers as combine_map_layers
from ecoscope_workflows_ext_ste.tasks import (
    create_view_state_from_gdf as create_view_state_from_gdf,
)
from ecoscope_workflows_ext_ste.tasks import (
    get_split_group_column as get_split_group_column,
)
from ecoscope_workflows_ext_ste.tasks import (
    label_quarter_status as label_quarter_status,
)
from ecoscope_workflows_ext_ste.tasks import (
    modify_quarter_status_colors as modify_quarter_status_colors,
)
from ecoscope_workflows_ext_ste.tasks import zip_grouped_by_key as zip_grouped_by_key

determine_season_windows = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="determine_season_windows",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_nunique as dataframe_column_nunique,
)
from ecoscope_workflows_core.tasks.analysis import (
    dataframe_column_sum as dataframe_column_sum,
)
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_single_value_widget_single_view as create_single_value_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_text_widget_single_view as create_text_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.skip import never as never
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_polygon_layer as create_polygon_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap as draw_ecomap
from ecoscope_workflows_ext_ecoscope.tasks.skip import (
    all_geometry_are_none as all_geometry_are_none,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ste.tasks import (
    build_mapbook_report_template as build_mapbook_report_template,
)
from ecoscope_workflows_ext_ste.tasks import (
    calculate_seasonal_home_range as calculate_seasonal_home_range,
)
from ecoscope_workflows_ext_ste.tasks import combine_map_layers as combine_map_layers
from ecoscope_workflows_ext_ste.tasks import create_context_page as create_context_page
from ecoscope_workflows_ext_ste.tasks import (
    create_mapbook_context as create_mapbook_context,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_report_context_from_tuple as create_report_context_from_tuple,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_seasonal_labels as create_seasonal_labels,
)
from ecoscope_workflows_ext_ste.tasks import (
    dataframe_column_first_unique_str as dataframe_column_first_unique_str,
)
from ecoscope_workflows_ext_ste.tasks import flatten_tuple as flatten_tuple
from ecoscope_workflows_ext_ste.tasks import (
    generate_ecograph_raster as generate_ecograph_raster,
)
from ecoscope_workflows_ext_ste.tasks import generate_mcp_gdf as generate_mcp_gdf
from ecoscope_workflows_ext_ste.tasks import get_duration as get_duration
from ecoscope_workflows_ext_ste.tasks import (
    get_split_group_names as get_split_group_names,
)
from ecoscope_workflows_ext_ste.tasks import merge_docx_files as merge_docx_files
from ecoscope_workflows_ext_ste.tasks import (
    retrieve_feature_gdf as retrieve_feature_gdf,
)
from ecoscope_workflows_ext_ste.tasks import round_off_values as round_off_values
from ecoscope_workflows_ext_ste.tasks import zip_grouped_by_key as zip_grouped_by_key
from ecoscope_workflows_ext_ste.tasks import zip_lists as zip_lists

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    initialize_workflow_metadata = (
        set_workflow_details.validate()
        .set_task_instance_id("initialize_workflow_metadata")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("initialize_workflow_metadata") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z",
            timezone={
                "label": "UTC",
                "tzCode": "UTC",
                "name": "UTC",
                "utc_offset": "+00:00",
            },
            **(params_dict.get("time_range") or {}),
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    configure_base_maps = (
        set_base_maps.validate()
        .set_task_instance_id("configure_base_maps")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("configure_base_maps") or {}))
        .call()
    )

    download_mapbook_cover_page = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_mapbook_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/1373gi65ji918rxele5h9/cover_page_v3.docx?rlkey=ur01wtpa98tcyq8f0f6dtksl8&st=eq39sgwz&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_mapbook_cover_page") or {}),
        )
        .call()
    )

    download_sect_templates = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_sect_templates")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/0as1u7uuhia7emp5cqxfl/mapbook_subject_template_v6.docx?rlkey=4nzn4qa2hu0v3fqo8bpki4tgu&st=kco28x6g&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_sect_templates") or {}),
        )
        .call()
    )

    download_logo_path = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_logo_path")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/1gn84pq9c7tedgg3k90qt/save-the-elephants.jpg?rlkey=ump7g2hcc2pn0pd5nst203c7w&st=jlwbhik9&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_logo_path") or {}),
        )
        .call()
    )

    retrieve_ldx_db = (
        get_file_path.validate()
        .set_task_instance_id("retrieve_ldx_db")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("retrieve_ldx_db") or {}),
        )
        .call()
    )

    load_landdx = (
        load_df.validate()
        .set_task_instance_id("load_landdx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=retrieve_ldx_db,
            layer="landDx_polygons",
            deserialize_json=False,
            **(params_dict.get("load_landdx") or {}),
        )
        .call()
    )

    filter_landdx_aoi = (
        filter_row_values.validate()
        .set_task_instance_id("filter_landdx_aoi")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=load_landdx,
            column="type",
            values=["Community Conservancy", "National Reserve", "National Park"],
            **(params_dict.get("filter_landdx_aoi") or {}),
        )
        .call()
    )

    custom_text_layer = (
        make_text_layer.validate()
        .set_task_instance_id("custom_text_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            txt_gdf=filter_landdx_aoi,
            label_column="label",
            name_column="name",
            use_centroid=True,
            color=[0, 0, 0, 255],
            size=16,
            font_weight="normal",
            font_family="Arial",
            text_anchor="middle",
            alignment_baseline="center",
            pickable=True,
            tooltip_columns=None,
            zoom=False,
            target_crs="epsg:4326",
            **(params_dict.get("custom_text_layer") or {}),
        )
        .call()
    )

    split_landdx_by_type = (
        split_gdf_by_column.validate()
        .set_task_instance_id("split_landdx_by_type")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=filter_landdx_aoi,
            column="type",
            **(params_dict.get("split_landdx_by_type") or {}),
        )
        .call()
    )

    annotate_geometry_types = (
        annotate_gdf_dict_with_geom_type.validate()
        .set_task_instance_id("annotate_geometry_types")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf_dict=split_landdx_by_type,
            **(params_dict.get("annotate_geometry_types") or {}),
        )
        .call()
    )

    create_styled_landdx_layers = (
        create_layers_from_gdf_dict.validate()
        .set_task_instance_id("create_styled_landdx_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf_dict=annotate_geometry_types,
            styles={
                "Community Conservancy": {
                    "get_fill_color": [85, 107, 47],
                    "get_line_color": [85, 107, 47],
                    "opacity": 0.15,
                    "stroked": True,
                    "get_line_width": 1.55,
                },
                "National Reserve": {
                    "get_fill_color": [143, 188, 139],
                    "get_line_color": [143, 188, 139],
                    "opacity": 0.15,
                    "stroked": True,
                    "get_line_width": 1.55,
                },
                "National Park": {
                    "get_fill_color": [255, 250, 205],
                    "get_line_color": [255, 250, 205],
                    "opacity": 0.15,
                    "stroked": True,
                    "get_line_width": 1.55,
                },
            },
            legends={
                "Community Conservancy": {
                    "labels": ["Community Conservancy"],
                    "colors": ["#556b2f"],
                },
                "National Reserve": {
                    "labels": ["National Reserve"],
                    "colors": ["#8fbc8b"],
                },
                "National Park": {"labels": ["National Park"], "colors": ["#fffacd"]},
            },
            **(params_dict.get("create_styled_landdx_layers") or {}),
        )
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    gee_project_name = (
        set_gee_connection.validate()
        .set_task_instance_id("gee_project_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("gee_project_name") or {}))
        .call()
    )

    subject_observations = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_observations")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_observations") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=subject_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__hex",
                "extra__subject__sex",
                "extra__created_at",
                "extra__subject__subject_subtype",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    annotate_day_night = (
        classify_is_night.validate()
        .set_task_instance_id("annotate_day_night")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=subject_reloc, **(params_dict.get("annotate_day_night") or {})
        )
        .call()
    )

    convert_to_trajectories = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("convert_to_trajectories")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=annotate_day_night,
            **(params_dict.get("convert_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_index_to_traj = (
        add_temporal_index.validate()
        .set_task_instance_id("add_temporal_index_to_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_to_trajectories,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_index_to_traj") or {}),
        )
        .call()
    )

    classify_trajectory_speed_bins = (
        apply_classification.validate()
        .set_task_instance_id("classify_trajectory_speed_bins")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_temporal_index_to_traj,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_trajectory_speed_bins") or {}),
        )
        .call()
    )

    label_trajectory_quarters = (
        label_quarter_status.validate()
        .set_task_instance_id("label_trajectory_quarters")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=classify_trajectory_speed_bins,
            timestamp_col="segment_start",
            **(params_dict.get("label_trajectory_quarters") or {}),
        )
        .call()
    )

    rename_traj_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_traj_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__hex": "hex_color",
                "extra__is_night": "is_night",
                "extra__name": "subject_name",
                "extra__sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
                "extra__created_at": "created_at",
            },
            df=label_trajectory_quarters,
            **(params_dict.get("rename_traj_cols") or {}),
        )
        .call()
    )

    persist_trajectory_df = (
        persist_df.validate()
        .set_task_instance_id("persist_trajectory_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            filetype="gpkg",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="trajectories",
            **(params_dict.get("persist_trajectory_df") or {}),
        )
        .call()
    )

    persist_trajectory_gpq = (
        persist_df.validate()
        .set_task_instance_id("persist_trajectory_gpq")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="trajectories",
            **(params_dict.get("persist_trajectory_gpq") or {}),
        )
        .call()
    )

    persist_relocs_df = (
        persist_df.validate()
        .set_task_instance_id("persist_relocs_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=annotate_day_night,
            filetype="gpkg",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="relocations",
            **(params_dict.get("persist_relocs_df") or {}),
        )
        .call()
    )

    persist_relocs_gpq = (
        persist_df.validate()
        .set_task_instance_id("persist_relocs_gpq")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=annotate_day_night,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="relocations",
            **(params_dict.get("persist_relocs_gpq") or {}),
        )
        .call()
    )

    split_trajectories_by_group = (
        split_groups.validate()
        .set_task_instance_id("split_trajectories_by_group")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            groupers=groupers,
            **(params_dict.get("split_trajectories_by_group") or {}),
        )
        .call()
    )

    split_group_column = (
        get_split_group_column.validate()
        .set_task_instance_id("split_group_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            split_data=split_trajectories_by_group,
            **(params_dict.get("split_group_column") or {}),
        )
        .call()
    )

    assign_quarter_colors_traj = (
        modify_quarter_status_colors.validate()
        .set_task_instance_id("assign_quarter_colors_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            grouper_value=split_group_column,
            **(params_dict.get("assign_quarter_colors_traj") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=split_trajectories_by_group)
    )

    sort_trajectories_by_speed = (
        sort_values.validate()
        .set_task_instance_id("sort_trajectories_by_speed")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="speed_bins",
            na_position="last",
            ascending=True,
            **(params_dict.get("sort_trajectories_by_speed") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=assign_quarter_colors_traj)
    )

    apply_speed_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_speed_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_speed_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_trajectories_by_speed)
    )

    format_speed_bin_labels = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_speed_bin_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_bins",
            output_column_name="speed_bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_bin_labels") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_speed_colormap)
    )

    format_speed_values = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_speed_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="speed_kmhr",
            output_column_name="speed_kmhr",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=format_speed_bin_labels)
    )

    generate_speedmap_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("generate_speedmap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "speed_bins_colormap"},
            legend={
                "label_column": "speed_bins_formatted",
                "color_column": "speed_bins_colormap",
            },
            tooltip_columns=[
                "is_night",
                "subject_name",
                "segment_start",
                "dist_meters",
                "timespan_seconds",
                "subject_sex",
            ],
            **(params_dict.get("generate_speedmap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=format_speed_values)
    )

    zoom_global_view = (
        create_view_state_from_gdf.validate()
        .set_task_instance_id("zoom_global_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_global_view") or {}))
        .mapvalues(argnames=["gdf"], argvalues=format_speed_values)
    )

    ldx_speed_layers = (
        combine_map_layers.validate()
        .set_task_instance_id("ldx_speed_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_styled_landdx_layers, custom_text_layer],
            **(params_dict.get("ldx_speed_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_speedmap_layers)
    )

    zip_speed_zoom_values = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_speed_zoom_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=ldx_speed_layers,
            right=zoom_global_view,
            **(params_dict.get("zip_speed_zoom_values") or {}),
        )
        .call()
    )

    draw_speed_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("draw_speed_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Speed Values(Km/h)"},
            static=False,
            title=None,
            max_zoom=9,
            **(params_dict.get("draw_speed_ecomap") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=zip_speed_zoom_values
        )
    )

    persist_speed_ecomap_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_speed_ecomap_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speedmap",
            **(params_dict.get("persist_speed_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_speed_ecomap)
    )

    create_speedmap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_speedmap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Speed Map", **(params_dict.get("create_speedmap_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_speed_ecomap_urls)
    )

    merge_speedmap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_speedmap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_speedmap_widgets,
            **(params_dict.get("merge_speedmap_widgets") or {}),
        )
        .call()
    )

    sort_trajs_by_day_night = (
        sort_values.validate()
        .set_task_instance_id("sort_trajs_by_day_night")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="is_night",
            ascending=False,
            na_position="last",
            **(params_dict.get("sort_trajs_by_day_night") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=assign_quarter_colors_traj)
    )

    apply_day_night_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_day_night_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            colormap=["#292965", "#e7a553"],
            input_column_name="is_night",
            output_column_name="day_night_colors",
            **(params_dict.get("apply_day_night_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_trajs_by_day_night)
    )

    generate_day_night_ecomap_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("generate_day_night_ecomap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "day_night_colors"},
            legend={"labels": ["Night", "Day"], "colors": ["#292965", "#e7a553"]},
            tooltip_columns=["is_night", "subject_name", "subject_sex"],
            **(params_dict.get("generate_day_night_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_day_night_colormap)
    )

    ldx_dn_layers = (
        combine_map_layers.validate()
        .set_task_instance_id("ldx_dn_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_styled_landdx_layers, custom_text_layer],
            **(params_dict.get("ldx_dn_layers") or {}),
        )
        .mapvalues(
            argnames=["grouped_layers"], argvalues=generate_day_night_ecomap_layers
        )
    )

    zoom_day_night = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zoom_day_night")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=ldx_dn_layers,
            right=zoom_global_view,
            **(params_dict.get("zoom_day_night") or {}),
        )
        .call()
    )

    draw_day_night_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("draw_day_night_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Night Day Tracks"},
            static=False,
            title=None,
            max_zoom=9,
            **(params_dict.get("draw_day_night_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=zoom_day_night)
    )

    persist_day_night_ecomap_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_day_night_ecomap_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="day_night",
            **(params_dict.get("persist_day_night_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_day_night_ecomap)
    )

    create_day_night_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_day_night_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Night Day Tracks",
            **(params_dict.get("create_day_night_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_day_night_ecomap_urls)
    )

    merge_day_night_ecomap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_day_night_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_day_night_ecomap_widgets,
            **(params_dict.get("merge_day_night_ecomap_widgets") or {}),
        )
        .call()
    )

    sort_trajs_by_quarter_status = (
        sort_values.validate()
        .set_task_instance_id("sort_trajs_by_quarter_status")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="quarter_status",
            ascending=False,
            na_position="last",
            **(params_dict.get("sort_trajs_by_quarter_status") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=assign_quarter_colors_traj)
    )

    generate_quarter_ecomap_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("generate_quarter_ecomap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"color_column": "quarter_status_colors"},
            legend={
                "label_column": "quarter_status",
                "color_column": "quarter_status_colors",
            },
            tooltip_columns=[
                "is_night",
                "subject_name",
                "subject_sex",
                "quarter_status",
            ],
            **(params_dict.get("generate_quarter_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=sort_trajs_by_quarter_status)
    )

    combine_quarter_ecomap_layers = (
        combine_map_layers.validate()
        .set_task_instance_id("combine_quarter_ecomap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_styled_landdx_layers, custom_text_layer],
            **(params_dict.get("combine_quarter_ecomap_layers") or {}),
        )
        .mapvalues(
            argnames=["grouped_layers"], argvalues=generate_quarter_ecomap_layers
        )
    )

    zoom_quarter_movements = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zoom_quarter_movements")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=combine_quarter_ecomap_layers,
            right=zoom_global_view,
            **(params_dict.get("zoom_quarter_movements") or {}),
        )
        .call()
    )

    draw_quarter_status_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("draw_quarter_status_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Legend"},
            static=False,
            title=None,
            max_zoom=9,
            **(params_dict.get("draw_quarter_status_ecomap") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=zoom_quarter_movements
        )
    )

    persist_quarter_ecomap_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_quarter_ecomap_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="movement_tracks",
            **(params_dict.get("persist_quarter_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_quarter_status_ecomap)
    )

    create_quarter_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_quarter_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Movement Overview",
            **(params_dict.get("create_quarter_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_quarter_ecomap_urls)
    )

    merge_quarter_ecomap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_quarter_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_quarter_ecomap_widgets,
            **(params_dict.get("merge_quarter_ecomap_widgets") or {}),
        )
        .call()
    )

    generate_etd = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("generate_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            auto_scale_or_custom_cell_size={
                "auto_scale_or_custom": "Customize",
                "grid_cell_size": 2000,
            },
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
            nodata_value="nan",
            band_count=1,
            max_speed_factor=1.05,
            expansion_factor=1.3,
            **(params_dict.get("generate_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=assign_quarter_colors_traj)
    )

    determine_seasonal_windows = (
        determine_season_windows.validate()
        .set_task_instance_id("determine_seasonal_windows")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=gee_project_name,
            time_range=time_range,
            **(params_dict.get("determine_seasonal_windows") or {}),
        )
        .mapvalues(argnames=["roi"], argvalues=generate_etd)
    )

    zip_etd_and_grouped_trajs = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_etd_and_grouped_trajs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=determine_seasonal_windows,
            right=assign_quarter_colors_traj,
            **(params_dict.get("zip_etd_and_grouped_trajs") or {}),
        )
        .call()
    )

    add_season_labels = (
        create_seasonal_labels.validate()
        .set_task_instance_id("add_season_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("add_season_labels") or {}))
        .mapvalues(
            argnames=["seasons_df", "trajectories"], argvalues=zip_etd_and_grouped_trajs
        )
    )

    calculate_mcp = (
        generate_mcp_gdf.validate()
        .set_task_instance_id("calculate_mcp")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(planar_crs="ESRI:53042", **(params_dict.get("calculate_mcp") or {}))
        .mapvalues(argnames=["gdf"], argvalues=assign_quarter_colors_traj)
    )

    apply_etd_percentile_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_etd_percentile_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("apply_etd_percentile_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_etd)
    )

    generate_etd_ecomap_layers = (
        create_polygon_layer.validate()
        .set_task_instance_id("generate_etd_ecomap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"fill_color_column": "percentile_colormap", "opacity": 0.55},
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("generate_etd_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_etd_percentile_colormap)
    )

    generate_mcp_layers = (
        create_polygon_layer.validate()
        .set_task_instance_id("generate_mcp_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": "#FFFFFF00",
                "get_line_color": "#ff1493",
                "get_line_width": 3.55,
                "opacity": 0.75,
                "stroked": True,
            },
            legend={"labels": ["MCP"], "colors": ["#ff1493"]},
            tooltip_columns=["area_km2"],
            **(params_dict.get("generate_mcp_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=calculate_mcp)
    )

    zip_mcp_hr = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_mcp_hr")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=generate_mcp_layers,
            right=generate_etd_ecomap_layers,
            **(params_dict.get("zip_mcp_hr") or {}),
        )
        .call()
    )

    combine_landdx_hr_ecomap_layers = (
        combine_map_layers.validate()
        .set_task_instance_id("combine_landdx_hr_ecomap_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_styled_landdx_layers, custom_text_layer],
            **(params_dict.get("combine_landdx_hr_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=zip_mcp_hr)
    )

    hr_view_zip = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("hr_view_zip")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=combine_landdx_hr_ecomap_layers,
            right=zoom_global_view,
            **(params_dict.get("hr_view_zip") or {}),
        )
        .call()
    )

    draw_hr_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("draw_hr_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "ETD Metrics"},
            static=False,
            title=None,
            max_zoom=9,
            **(params_dict.get("draw_hr_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=hr_view_zip)
    )

    persist_hr_ecomap_urls = (
        persist_text.validate()
        .set_task_instance_id("persist_hr_ecomap_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="home_range",
            **(params_dict.get("persist_hr_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_hr_ecomap)
    )

    create_hr_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("create_hr_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Home Range", **(params_dict.get("create_hr_ecomap_widgets") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_hr_ecomap_urls)
    )

    merge_hr_ecomap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("merge_hr_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=create_hr_ecomap_widgets,
            **(params_dict.get("merge_hr_ecomap_widgets") or {}),
        )
        .call()
    )

    generate_speed_raster = (
        generate_ecograph_raster.validate()
        .set_task_instance_id("generate_speed_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            step_length=2000,
            dist_col="dist_meters",
            interpolation="mean",
            movement_covariate="speed",
            radius=2,
            cutoff=None,
            tortuosity_length=3,
            resolution=None,
            network_metric=None,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename=None,
            **(params_dict.get("generate_speed_raster") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=assign_quarter_colors_traj)
    )

    extract_speed_rasters = (
        retrieve_feature_gdf.validate()
        .set_task_instance_id("extract_speed_rasters")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("extract_speed_rasters") or {}))
        .mapvalues(argnames=["file_path"], argvalues=generate_speed_raster)
    )

    sort_speed_features_by_value = (
        sort_values.validate()
        .set_task_instance_id("sort_speed_features_by_value")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="value",
            na_position="last",
            ascending=True,
            **(params_dict.get("sort_speed_features_by_value") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=extract_speed_rasters)
    )

    classify_speed_features = (
        apply_classification.validate()
        .set_task_instance_id("classify_speed_features")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value",
            output_column_name="bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_speed_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_speed_features_by_value)
    )

    apply_speed_raster_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_speed_raster_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="bins",
            output_column_name="speedraster_bins_colors",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_speed_raster_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=classify_speed_features)
    )

    format_speed_raster_labels = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_speed_raster_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="bins",
            output_column_name="bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_speed_raster_labels") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_speed_raster_colormap)
    )

    generate_raster_layers = (
        create_polygon_layer.validate()
        .set_task_instance_id("generate_raster_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "speedraster_bins_colors",
                "opacity": 0.55,
            },
            legend={
                "label_column": "bins_formatted",
                "color_column": "speedraster_bins_colors",
            },
            tooltip_columns=["value", "bins", "speedraster_bins_colors"],
            **(params_dict.get("generate_raster_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=format_speed_raster_labels)
    )

    combine_seasonal_raster_layers = (
        combine_map_layers.validate()
        .set_task_instance_id("combine_seasonal_raster_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_styled_landdx_layers, custom_text_layer],
            **(params_dict.get("combine_seasonal_raster_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_raster_layers)
    )

    speedraster_view_zip = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("speedraster_view_zip")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=combine_seasonal_raster_layers,
            right=zoom_global_view,
            **(params_dict.get("speedraster_view_zip") or {}),
        )
        .call()
    )

    draw_speed_raster_ecomaps = (
        draw_ecomap.validate()
        .set_task_instance_id("draw_speed_raster_ecomaps")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "placement": "bottom-right",
                "title": "Mean Speed Value (km/h)",
            },
            static=False,
            title=None,
            max_zoom=9,
            **(params_dict.get("draw_speed_raster_ecomaps") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"], argvalues=speedraster_view_zip
        )
    )

    speed_raster_ecomap_urls = (
        persist_text.validate()
        .set_task_instance_id("speed_raster_ecomap_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="speed_raster",
            **(params_dict.get("speed_raster_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_speed_raster_ecomaps)
    )

    speed_raster_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("speed_raster_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Mean Speed Map",
            **(params_dict.get("speed_raster_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=speed_raster_ecomap_urls)
    )

    speedraster_ecomap_widgets = (
        merge_widget_views.validate()
        .set_task_instance_id("speedraster_ecomap_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=speed_raster_ecomap_widgets,
            **(params_dict.get("speedraster_ecomap_widgets") or {}),
        )
        .call()
    )

    seasonal_home_range = (
        calculate_seasonal_home_range.validate()
        .set_task_instance_id("seasonal_home_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
                all_geometry_are_none,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["season"],
            percentiles=[99.9],
            auto_scale_or_custom_cell_size={
                "auto_scale_or_custom": "Customize",
                "grid_cell_size": 2000,
            },
            **(params_dict.get("seasonal_home_range") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=add_season_labels)
    )

    season_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("season_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="season",
            output_column_name="season_colormap",
            colormap=["#f57c00", "#255084"],
            **(params_dict.get("season_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
    )

    season_etd_map_layer = (
        create_polygon_layer.validate()
        .set_task_instance_id("season_etd_map_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"fill_color_column": "season_colormap", "opacity": 0.65},
            legend={"label_column": "season", "color_column": "season_colormap"},
            tooltip_columns=["percentile"],
            **(params_dict.get("season_etd_map_layer") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=season_colormap)
    )

    comb_season_map_layers = (
        combine_map_layers.validate()
        .set_task_instance_id("comb_season_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_styled_landdx_layers, custom_text_layer],
            **(params_dict.get("comb_season_map_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=season_etd_map_layer)
    )

    seasons_view_zip = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("seasons_view_zip")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            left=comb_season_map_layers,
            right=zoom_global_view,
            **(params_dict.get("seasons_view_zip") or {}),
        )
        .call()
    )

    seasonal_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("seasonal_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Seasons"},
            static=False,
            title=None,
            max_zoom=9,
            **(params_dict.get("seasonal_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=seasons_view_zip)
    )

    season_etd_ecomap_html_url = (
        persist_text.validate()
        .set_task_instance_id("season_etd_ecomap_html_url")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename_suffix="seasons",
            **(params_dict.get("season_etd_ecomap_html_url") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=seasonal_ecomap)
    )

    season_etd_widgets_single_view = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("season_etd_widgets_single_view")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Seasonal Home Range",
            **(params_dict.get("season_etd_widgets_single_view") or {}),
        )
        .map(argnames=["view", "data"], argvalues=season_etd_ecomap_html_url)
    )

    season_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("season_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=season_etd_widgets_single_view,
            **(params_dict.get("season_grouped_map_widget") or {}),
        )
        .call()
    )

    total_mcp_area = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_mcp_area")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="area_km2", **(params_dict.get("total_mcp_area") or {}))
        .mapvalues(argnames=["df"], argvalues=calculate_mcp)
    )

    round_mcp_area = (
        round_off_values.validate()
        .set_task_instance_id("round_mcp_area")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=2, **(params_dict.get("round_mcp_area") or {}))
        .mapvalues(argnames=["value"], argvalues=total_mcp_area)
    )

    total_grid_area = (
        dataframe_column_sum.validate()
        .set_task_instance_id("total_grid_area")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="area_sqkm", **(params_dict.get("total_grid_area") or {}))
        .mapvalues(argnames=["df"], argvalues=generate_etd)
    )

    round_grid_area = (
        round_off_values.validate()
        .set_task_instance_id("round_grid_area")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(dp=1, **(params_dict.get("round_grid_area") or {}))
        .mapvalues(argnames=["value"], argvalues=total_grid_area)
    )

    total_mcp_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_mcp_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total MCP Area (Km2)",
            decimal_places=1,
            **(params_dict.get("total_mcp_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_mcp_area)
    )

    total_mcp_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_mcp_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_mcp_sv_widgets,
            **(params_dict.get("total_mcp_grouped_sv_widget") or {}),
        )
        .call()
    )

    total_grid_sv_widgets = (
        create_single_value_widget_single_view.validate()
        .set_task_instance_id("total_grid_sv_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Total Grid Area(Km2)",
            decimal_places=1,
            **(params_dict.get("total_grid_sv_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=round_grid_area)
    )

    total_grid_grouped_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("total_grid_grouped_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=total_grid_sv_widgets,
            **(params_dict.get("total_grid_grouped_sv_widget") or {}),
        )
        .call()
    )

    subject_gender = (
        dataframe_column_first_unique_str.validate()
        .set_task_instance_id("subject_gender")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(column_name="subject_sex", **(params_dict.get("subject_gender") or {}))
        .mapvalues(argnames=["df"], argvalues=assign_quarter_colors_traj)
    )

    gender_widgets = (
        create_text_widget_single_view.validate()
        .set_task_instance_id("gender_widgets")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(title="Gender", **(params_dict.get("gender_widgets") or {}))
        .map(argnames=["view", "data"], argvalues=subject_gender)
    )

    gender_sv_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("gender_sv_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(widgets=gender_widgets, **(params_dict.get("gender_sv_widget") or {}))
        .call()
    )

    report_duration = (
        get_duration.validate()
        .set_task_instance_id("report_duration")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_range=time_range,
            time_unit="months",
            **(params_dict.get("report_duration") or {}),
        )
        .call()
    )

    round_report_duration = (
        round_off_values.validate()
        .set_task_instance_id("round_report_duration")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            dp=2,
            value=report_duration,
            **(params_dict.get("round_report_duration") or {}),
        )
        .call()
    )

    get_subject_name = (
        dataframe_column_first_unique_str.validate()
        .set_task_instance_id("get_subject_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="subject_name", **(params_dict.get("get_subject_name") or {})
        )
        .mapvalues(argnames=["df"], argvalues=assign_quarter_colors_traj)
    )

    unique_subjects = (
        dataframe_column_nunique.validate()
        .set_task_instance_id("unique_subjects")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            column_name="subject_name",
            **(params_dict.get("unique_subjects") or {}),
        )
        .call()
    )

    create_cover_template_context = (
        build_mapbook_report_template.validate()
        .set_task_instance_id("create_cover_template_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            count=unique_subjects,
            org_logo_path=download_logo_path,
            report_period=time_range,
            prepared_by="Ecoscope",
            **(params_dict.get("create_cover_template_context") or {}),
        )
        .call()
    )

    persist_context_cover = (
        create_context_page.validate()
        .set_task_instance_id("persist_context_cover")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            logo_width_cm=4.5,
            logo_height_cm=1.93,
            template_path=download_mapbook_cover_page,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context=create_cover_template_context,
            filename=None,
            **(params_dict.get("persist_context_cover") or {}),
        )
        .call()
    )

    convert_speedmap_html_to_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_speedmap_html_to_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 40000},
            **(params_dict.get("convert_speedmap_html_to_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_speed_ecomap_urls)
    )

    convert_day_night_html_to_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_day_night_html_to_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 40000},
            **(params_dict.get("convert_day_night_html_to_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_day_night_ecomap_urls)
    )

    convert_quarter_html_to_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_quarter_html_to_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 40000},
            **(params_dict.get("convert_quarter_html_to_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_quarter_ecomap_urls)
    )

    convert_hr_html_to_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_hr_html_to_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 40000},
            **(params_dict.get("convert_hr_html_to_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_hr_ecomap_urls)
    )

    convert_speed_raster_html_to_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_speed_raster_html_to_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 40000},
            **(params_dict.get("convert_speed_raster_html_to_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=speed_raster_ecomap_urls)
    )

    convert_seasonal_hr_html_to_png = (
        html_to_png.validate()
        .set_task_instance_id("convert_seasonal_hr_html_to_png")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={"wait_for_timeout": 40000},
            **(params_dict.get("convert_seasonal_hr_html_to_png") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=season_etd_ecomap_html_url)
    )

    zip_grid_mcp = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_grid_mcp")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=round_grid_area,
            right=round_mcp_area,
            **(params_dict.get("zip_grid_mcp") or {}),
        )
        .call()
    )

    zip_grid_mcp_quarter = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_grid_mcp_quarter")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_grid_mcp,
            right=convert_quarter_html_to_png,
            **(params_dict.get("zip_grid_mcp_quarter") or {}),
        )
        .call()
    )

    zip_grid_mcp_quarter_hr = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_grid_mcp_quarter_hr")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_grid_mcp_quarter,
            right=convert_hr_html_to_png,
            **(params_dict.get("zip_grid_mcp_quarter_hr") or {}),
        )
        .call()
    )

    zip_grid_mcp_speedraster = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_grid_mcp_speedraster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_grid_mcp_quarter_hr,
            right=convert_speed_raster_html_to_png,
            **(params_dict.get("zip_grid_mcp_speedraster") or {}),
        )
        .call()
    )

    zip_grid_mcp_dn = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_grid_mcp_dn")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_grid_mcp_speedraster,
            right=convert_day_night_html_to_png,
            **(params_dict.get("zip_grid_mcp_dn") or {}),
        )
        .call()
    )

    zip_grid_dn_speedmap = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_grid_dn_speedmap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_grid_mcp_dn,
            right=convert_speedmap_html_to_png,
            **(params_dict.get("zip_grid_dn_speedmap") or {}),
        )
        .call()
    )

    zip_all_mapbook_context_inputs = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_all_mapbook_context_inputs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_grid_dn_speedmap,
            right=convert_seasonal_hr_html_to_png,
            **(params_dict.get("zip_all_mapbook_context_inputs") or {}),
        )
        .call()
    )

    zip_all_with_name = (
        zip_grouped_by_key.validate()
        .set_task_instance_id("zip_all_with_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=zip_all_mapbook_context_inputs,
            right=get_subject_name,
            **(params_dict.get("zip_all_with_name") or {}),
        )
        .call()
    )

    flatten_mbook_context = (
        flatten_tuple.validate()
        .set_task_instance_id("flatten_mbook_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("flatten_mbook_context") or {}))
        .mapvalues(argnames=["nested"], argvalues=zip_all_with_name)
    )

    get_grouper_names = (
        get_split_group_names.validate()
        .set_task_instance_id("get_grouper_names")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            split_data=split_trajectories_by_group,
            **(params_dict.get("get_grouper_names") or {}),
        )
        .call()
    )

    zip_grouper_with_context = (
        zip_lists.validate()
        .set_task_instance_id("zip_grouper_with_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            left=get_grouper_names,
            right=flatten_mbook_context,
            **(params_dict.get("zip_grouper_with_context") or {}),
        )
        .call()
    )

    flatten_final_report_context = (
        flatten_tuple.validate()
        .set_task_instance_id("flatten_final_report_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("flatten_final_report_context") or {}))
        .mapvalues(argnames=["nested"], argvalues=zip_grouper_with_context)
    )

    prepare_mapbook_context = (
        create_report_context_from_tuple.validate()
        .set_task_instance_id("prepare_mapbook_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("prepare_mapbook_context") or {}))
        .mapvalues(
            argnames=[
                "grouper_type",
                "grouper_eq",
                "grouper_value",
                "grid_area",
                "mcp_area",
                "movement_tracks_ecomap",
                "home_range_ecomap",
                "speed_raster_ecomap",
                "night_day_ecomap",
                "speedmap",
                "seasonal_homerange",
                "subject_name",
            ],
            argvalues=flatten_final_report_context,
        )
    )

    individual_mapbook_context = (
        create_mapbook_context.validate()
        .set_task_instance_id("individual_mapbook_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_sect_templates,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            time_period=time_range,
            period=round_report_duration,
            filename=None,
            validate_images=True,
            box_h_cm=6.5,
            box_w_cm=11.11,
            **(params_dict.get("individual_mapbook_context") or {}),
        )
        .mapvalues(argnames=["context"], argvalues=prepare_mapbook_context)
    )

    generate_mapbook_report = (
        merge_docx_files.validate()
        .set_task_instance_id("generate_mapbook_report")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=persist_context_cover,
            output_directory=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=individual_mapbook_context,
            filename="mapbook_report.docx",
            **(params_dict.get("generate_mapbook_report") or {}),
        )
        .call()
    )

    mapbook_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("mapbook_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=initialize_workflow_metadata,
            widgets=[
                gender_sv_widget,
                total_mcp_grouped_sv_widget,
                total_grid_grouped_sv_widget,
                merge_quarter_ecomap_widgets,
                merge_hr_ecomap_widgets,
                merge_speedmap_widgets,
                speedraster_ecomap_widgets,
                merge_day_night_ecomap_widgets,
                season_grouped_map_widget,
            ],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("mapbook_dashboard") or {}),
        )
        .call()
    )

    return mapbook_dashboard
