# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.config import set_workflow_details
from ecoscope_workflows_core.tasks.filter import set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps
from ecoscope_workflows_ext_ste.tasks import create_directory
from ecoscope_workflows_ext_ste.tasks import download_land_dx
from ecoscope_workflows_ext_ste.tasks import load_landdx_aoi
from ecoscope_workflows_ext_ste.tasks import split_gdf_by_column
from ecoscope_workflows_ext_ste.tasks import annotate_gdf_dict_with_geometry_type
from ecoscope_workflows_ext_ste.tasks import create_map_layers_from_annotated_dict
from ecoscope_workflows_core.tasks.io import set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection
from ecoscope_workflows_ext_ecoscope.tasks.io import get_subjectgroup_observations
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import process_relocations
from ecoscope_workflows_ext_ecoscope.tasks.transformation import classify_is_night
from ecoscope_workflows_ext_ste.tasks import add_day_night_column
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory,
)
from ecoscope_workflows_core.tasks.transformation import add_temporal_index
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_classification
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import determine_season_windows
from ecoscope_workflows_ext_ste.tasks import create_seasonal_labels
from ecoscope_workflows_ext_ste.tasks import convert_col_to_rgba
from ecoscope_workflows_ext_ecoscope.tasks.analysis import create_meshgrid
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df
from ecoscope_workflows_ext_ste.tasks import assign_column
from ecoscope_workflows_ext_ste.tasks import spatial_join
from ecoscope_workflows_ext_ste.tasks import assign_value_by_index
from ecoscope_workflows_core.tasks.groupby import split_groups
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polyline_layer
from ecoscope_workflows_core.tasks.skip import any_is_empty_df
from ecoscope_workflows_core.tasks.skip import any_dependency_skipped
from ecoscope_workflows_ext_ste.tasks import combine_map_layers
from ecoscope_workflows_ext_ste.tasks import create_view_state_from_gdf
from ecoscope_workflows_ext_ste.tasks import zip_grouped_by_key
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap
from ecoscope_workflows_core.tasks.io import persist_text
from ecoscope_workflows_core.tasks.results import create_map_widget_single_view
from ecoscope_workflows_core.tasks.skip import never
from ecoscope_workflows_core.tasks.results import merge_widget_views
from ecoscope_workflows_ext_ecoscope.tasks.transformation import apply_color_map
from ecoscope_workflows_ext_ecoscope.tasks.results import create_polygon_layer
from ecoscope_workflows_ext_ste.tasks import filter_column_values
from ecoscope_workflows_ext_ste.tasks import calculate_etd_by_groups
from ecoscope_workflows_core.tasks.transformation import sort_values
from ecoscope_workflows_ext_ste.tasks import compute_fix_density_and_night_class
from ecoscope_workflows_ext_ste.tasks import filter_time_dominance
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    drop_nan_values_by_column,
)
from ecoscope_workflows_ext_ste.tasks import drop_missing_values_by_column
from ecoscope_workflows_ext_ste.tasks import generate_ecograph_raster
from ecoscope_workflows_ext_ste.tasks import retrieve_feature_gdf
from ecoscope_workflows_core.tasks.transformation import map_values_with_unit
from ecoscope_workflows_ext_ste.tasks import category_summary
from ecoscope_workflows_ext_ste.tasks import plot_protected_fix_proportions
from ecoscope_workflows_core.tasks.results import create_plot_widget_single_view
from ecoscope_workflows_core.tasks.results import gather_dashboard

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    initialize_workflow_metadata = (
        set_workflow_details.validate()
        .handle_errors(task_instance_id="initialize_workflow_metadata")
        .partial(**(params_dict.get("initialize_workflow_metadata") or {}))
        .call()
    )

    define_time_range = (
        set_time_range.validate()
        .handle_errors(task_instance_id="define_time_range")
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z",
            **(params_dict.get("define_time_range") or {}),
        )
        .call()
    )

    configure_grouping_strategy = (
        set_groupers.validate()
        .handle_errors(task_instance_id="configure_grouping_strategy")
        .partial(**(params_dict.get("configure_grouping_strategy") or {}))
        .call()
    )

    configure_base_maps = (
        set_base_maps.validate()
        .handle_errors(task_instance_id="configure_base_maps")
        .partial(**(params_dict.get("configure_base_maps") or {}))
        .call()
    )

    create_output_directory = (
        create_directory.validate()
        .handle_errors(task_instance_id="create_output_directory")
        .partial(**(params_dict.get("create_output_directory") or {}))
        .call()
    )

    retrieve_landdx_database = (
        download_land_dx.validate()
        .handle_errors(task_instance_id="retrieve_landdx_database")
        .partial(
            path=create_output_directory,
            **(params_dict.get("retrieve_landdx_database") or {}),
        )
        .call()
    )

    load_aoi = (
        load_landdx_aoi.validate()
        .handle_errors(task_instance_id="load_aoi")
        .partial(
            map_path=retrieve_landdx_database, **(params_dict.get("load_aoi") or {})
        )
        .call()
    )

    split_landdx_by_type = (
        split_gdf_by_column.validate()
        .handle_errors(task_instance_id="split_landdx_by_type")
        .partial(
            gdf=load_aoi,
            column="type",
            **(params_dict.get("split_landdx_by_type") or {}),
        )
        .call()
    )

    annotate_geometry_types = (
        annotate_gdf_dict_with_geometry_type.validate()
        .handle_errors(task_instance_id="annotate_geometry_types")
        .partial(
            gdf_dict=split_landdx_by_type,
            **(params_dict.get("annotate_geometry_types") or {}),
        )
        .call()
    )

    create_styled_landdx_layers = (
        create_map_layers_from_annotated_dict.validate()
        .handle_errors(task_instance_id="create_styled_landdx_layers")
        .partial(
            annotated_dict=annotate_geometry_types,
            **(params_dict.get("create_styled_landdx_layers") or {}),
        )
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .handle_errors(task_instance_id="er_client_name")
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    gee_project_name = (
        set_gee_connection.validate()
        .handle_errors(task_instance_id="gee_project_name")
        .partial(**(params_dict.get("gee_project_name") or {}))
        .call()
    )

    subject_observations = (
        get_subjectgroup_observations.validate()
        .handle_errors(task_instance_id="subject_observations")
        .partial(
            client=er_client_name,
            time_range=define_time_range,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_observations") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .handle_errors(task_instance_id="subject_reloc")
        .partial(
            observations=subject_observations,
            relocs_columns=[
                "fixtime",
                "geometry",
                "groupby_col",
                "junk_status",
                "extra__created_at",
                "extra__subject__sex",
                "extra__subject__hex",
                "extra__subject__name",
                "extra__subject__subject_subtype",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    annotate_day_night = (
        classify_is_night.validate()
        .handle_errors(task_instance_id="annotate_day_night")
        .partial(
            relocations=subject_reloc, **(params_dict.get("annotate_day_night") or {})
        )
        .call()
    )

    day_night_column = (
        add_day_night_column.validate()
        .handle_errors(task_instance_id="day_night_column")
        .partial(
            source_col="is_night",
            df=annotate_day_night,
            **(params_dict.get("day_night_column") or {}),
        )
        .call()
    )

    convert_to_trajectories = (
        relocations_to_trajectory.validate()
        .handle_errors(task_instance_id="convert_to_trajectories")
        .partial(
            relocations=day_night_column,
            **(params_dict.get("convert_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_index_to_traj = (
        add_temporal_index.validate()
        .handle_errors(task_instance_id="add_temporal_index_to_traj")
        .partial(
            df=convert_to_trajectories,
            time_col="segment_start",
            groupers=configure_grouping_strategy,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_index_to_traj") or {}),
        )
        .call()
    )

    classify_trajectory_speed_bins = (
        apply_classification.validate()
        .handle_errors(task_instance_id="classify_trajectory_speed_bins")
        .partial(
            df=add_temporal_index_to_traj,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_trajectory_speed_bins") or {}),
        )
        .call()
    )

    generate_seasonal_etd = (
        calculate_elliptical_time_density.validate()
        .handle_errors(task_instance_id="generate_seasonal_etd")
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.0],
            nodata_value="nan",
            band_count=1,
            trajectory_gdf=classify_trajectory_speed_bins,
            **(params_dict.get("generate_seasonal_etd") or {}),
        )
        .call()
    )

    determine_seasonal_windows = (
        determine_season_windows.validate()
        .handle_errors(task_instance_id="determine_seasonal_windows")
        .partial(
            client=gee_project_name,
            roi=generate_seasonal_etd,
            time_range=define_time_range,
            **(params_dict.get("determine_seasonal_windows") or {}),
        )
        .call()
    )

    add_season_labels = (
        create_seasonal_labels.validate()
        .handle_errors(task_instance_id="add_season_labels")
        .partial(
            traj=classify_trajectory_speed_bins,
            total_percentiles=determine_seasonal_windows,
            **(params_dict.get("add_season_labels") or {}),
        )
        .call()
    )

    convert_hex_rgba = (
        convert_col_to_rgba.validate()
        .handle_errors(task_instance_id="convert_hex_rgba")
        .partial(
            df=add_season_labels,
            col="extra__hex",
            new_col="hex_colors",
            **(params_dict.get("convert_hex_rgba") or {}),
        )
        .call()
    )

    create_dn_meshgrid = (
        create_meshgrid.validate()
        .handle_errors(task_instance_id="create_dn_meshgrid")
        .partial(
            intersecting_only=False,
            crs="EPSG:3857",
            aoi=day_night_column,
            **(params_dict.get("create_dn_meshgrid") or {}),
        )
        .call()
    )

    persist_traj_df = (
        persist_df.validate()
        .handle_errors(task_instance_id="persist_traj_df")
        .partial(
            df=convert_hex_rgba,
            filetype="gpkg",
            root_path=create_output_directory,
            **(params_dict.get("persist_traj_df") or {}),
        )
        .call()
    )

    persist_relocs_df = (
        persist_df.validate()
        .handle_errors(task_instance_id="persist_relocs_df")
        .partial(
            df=day_night_column,
            filetype="gpkg",
            root_path=create_output_directory,
            **(params_dict.get("persist_relocs_df") or {}),
        )
        .call()
    )

    assign_cols = (
        assign_column.validate()
        .handle_errors(task_instance_id="assign_cols")
        .partial(
            column_name="area_status",
            value="unprotected",
            df=convert_hex_rgba,
            **(params_dict.get("assign_cols") or {}),
        )
        .call()
    )

    join_traj_landdx = (
        spatial_join.validate()
        .handle_errors(task_instance_id="join_traj_landdx")
        .partial(
            how="inner",
            predicate="intersects",
            local_gdf=load_aoi,
            trajs_gdf=assign_cols,
            **(params_dict.get("join_traj_landdx") or {}),
        )
        .call()
    )

    assign_protected_values = (
        assign_value_by_index.validate()
        .handle_errors(task_instance_id="assign_protected_values")
        .partial(
            df=assign_cols,
            subset_df=join_traj_landdx,
            column_name="area_status",
            value="protected",
            **(params_dict.get("assign_protected_values") or {}),
        )
        .call()
    )

    split_trajectories_by_group = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_trajectories_by_group")
        .partial(
            df=assign_protected_values,
            groupers=configure_grouping_strategy,
            **(params_dict.get("split_trajectories_by_group") or {}),
        )
        .call()
    )

    split_relocations_by_group = (
        split_groups.validate()
        .handle_errors(task_instance_id="split_relocations_by_group")
        .partial(
            df=day_night_column,
            groupers=configure_grouping_strategy,
            **(params_dict.get("split_relocations_by_group") or {}),
        )
        .call()
    )

    generate_substyled_layers = (
        create_polyline_layer.validate()
        .handle_errors(task_instance_id="generate_substyled_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "color_column": "hex_colors",
                "auto_highlight": True,
                "pickable": True,
                "get_width": 3.0,
                "width_unit": "pixels",
                "cap_rounded": True,
                "opacity": 0.25,
            },
            legend={"label_column": "extra__name", "color_column": "extra__hex"},
            tooltip_columns=["extra__hex", "extra__name", "extra__sex"],
            **(params_dict.get("generate_substyled_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=split_trajectories_by_group)
    )

    combine_substyled_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_substyled_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_substyled_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_substyled_layers)
    )

    zoom_traj_view = (
        create_view_state_from_gdf.validate()
        .handle_errors(task_instance_id="zoom_traj_view")
        .partial(pitch=0, bearing=0, **(params_dict.get("zoom_traj_view") or {}))
        .mapvalues(argnames=["gdf"], argvalues=split_trajectories_by_group)
    )

    substyled_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="substyled_view_zip")
        .partial(
            left=combine_substyled_layers,
            right=zoom_traj_view,
            **(params_dict.get("substyled_view_zip") or {}),
        )
        .call()
    )

    draw_substyled_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_substyled_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Subject Tracks"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_substyled_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=substyled_view_zip)
    )

    persist_substyled_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_substyled_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_substyled_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_substyled_ecomaps)
    )

    create_substyled_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_substyled_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Subject Tracks Ecomaps",
            **(params_dict.get("create_substyled_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_substyled_urls)
    )

    merge_substyled_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_substyled_widgets")
        .partial(
            widgets=create_substyled_ecomap_widgets,
            **(params_dict.get("merge_substyled_widgets") or {}),
        )
        .call()
    )

    generate_etd = (
        calculate_elliptical_time_density.validate()
        .handle_errors(task_instance_id="generate_etd")
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.0],
            nodata_value="nan",
            band_count=1,
            **(params_dict.get("generate_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_trajectories_by_group)
    )

    td_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="td_colormap")
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("td_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_etd)
    )

    generate_etd_ecomap_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_etd_ecomap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "percentile_colormap",
                "opacity": 0.25,
                "stroked": False,
            },
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("generate_etd_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=td_colormap)
    )

    combine_landdx_hr_ecomap_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_hr_ecomap_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_landdx_hr_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_etd_ecomap_layers)
    )

    hr_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="hr_view_zip")
        .partial(
            left=combine_landdx_hr_ecomap_layers,
            right=zoom_traj_view,
            **(params_dict.get("hr_view_zip") or {}),
        )
        .call()
    )

    draw_hr_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_hr_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Home Range Metrics"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_hr_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=hr_view_zip)
    )

    persist_hr_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_hr_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_hr_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_hr_ecomaps)
    )

    create_hr_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_hr_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Home Range Ecomap",
            **(params_dict.get("create_hr_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_hr_ecomap_urls)
    )

    merge_hr_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_hr_ecomap_widgets")
        .partial(
            widgets=create_hr_ecomap_widgets,
            **(params_dict.get("merge_hr_ecomap_widgets") or {}),
        )
        .call()
    )

    custom_hr = (
        filter_column_values.validate()
        .handle_errors(task_instance_id="custom_hr")
        .partial(
            column="percentile", values=[99.0], **(params_dict.get("custom_hr") or {})
        )
        .mapvalues(argnames=["df"], argvalues=generate_etd)
    )

    custom_td_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="custom_td_colormap")
        .partial(
            input_column_name="percentile",
            colormap=["#d2691e"],
            output_column_name="percentile_colormap",
            **(params_dict.get("custom_td_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=custom_hr)
    )

    generate_cetd_ecomap_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_cetd_ecomap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "percentile_colormap",
                "opacity": 0.25,
                "stroked": False,
            },
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("generate_cetd_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=custom_td_colormap)
    )

    combine_landdx_custom_hr_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_custom_hr_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_landdx_custom_hr_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_cetd_ecomap_layers)
    )

    custom_hr_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="custom_hr_view_zip")
        .partial(
            left=combine_landdx_custom_hr_layers,
            right=zoom_traj_view,
            **(params_dict.get("custom_hr_view_zip") or {}),
        )
        .call()
    )

    draw_custom_hr_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_custom_hr_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Home Range Metrics"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_custom_hr_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=custom_hr_view_zip)
    )

    persist_custom_hr_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_custom_hr_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_custom_hr_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_custom_hr_ecomaps)
    )

    create_custom_hr_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_custom_hr_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="99th Percentile Home Range Ecomap",
            **(params_dict.get("create_custom_hr_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_custom_hr_ecomap_urls)
    )

    merge_custom_hr_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_custom_hr_ecomap_widgets")
        .partial(
            widgets=create_custom_hr_ecomap_widgets,
            **(params_dict.get("merge_custom_hr_ecomap_widgets") or {}),
        )
        .call()
    )

    generate_season_etd = (
        calculate_etd_by_groups.validate()
        .handle_errors(task_instance_id="generate_season_etd")
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 99.0],
            nodata_value="nan",
            band_count=1,
            include_groups=True,
            groupby_cols=["season"],
            **(params_dict.get("generate_season_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=split_trajectories_by_group)
    )

    sort_season_etd_color = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_season_etd_color")
        .partial(
            column_name="season",
            ascending=False,
            na_position="last",
            **(params_dict.get("sort_season_etd_color") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_season_etd)
    )

    apply_season_etd_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_season_etd_colormap")
        .partial(
            input_column_name="season",
            colormap=["#483d8b", "#deb887"],
            output_column_name="season_colormap",
            **(params_dict.get("apply_season_etd_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_season_etd_color)
    )

    generate_season_ecomap_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_season_ecomap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "season_colormap",
                "opacity": 0.25,
                "stroked": False,
            },
            legend={"label_column": "season", "color_column": "season_colormap"},
            tooltip_columns=["season"],
            **(params_dict.get("generate_season_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_season_etd_colormap)
    )

    combine_landdx_season_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_season_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_landdx_season_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_season_ecomap_layers)
    )

    season_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="season_view_zip")
        .partial(
            left=combine_landdx_season_layers,
            right=zoom_traj_view,
            **(params_dict.get("season_view_zip") or {}),
        )
        .call()
    )

    draw_season_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_season_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Seasons"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_season_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=season_view_zip)
    )

    persist_season_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_season_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_season_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_season_ecomaps)
    )

    create_season_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_season_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Seasonal Ecomap",
            **(params_dict.get("create_season_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_season_ecomap_urls)
    )

    merge_season_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_season_ecomap_widgets")
        .partial(
            widgets=create_season_ecomap_widgets,
            **(params_dict.get("merge_season_ecomap_widgets") or {}),
        )
        .call()
    )

    time_dominance = (
        compute_fix_density_and_night_class.validate()
        .handle_errors(task_instance_id="time_dominance")
        .partial(
            mode="dominance",
            grid=create_dn_meshgrid,
            geometry_type="point",
            **(params_dict.get("time_dominance") or {}),
        )
        .mapvalues(argnames=["selection"], argvalues=split_relocations_by_group)
    )

    filter_dominance_values = (
        filter_time_dominance.validate()
        .handle_errors(task_instance_id="filter_dominance_values")
        .partial(
            valid_dominance=["day", "night"],
            **(params_dict.get("filter_dominance_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=time_dominance)
    )

    sort_dominance_values = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_dominance_values")
        .partial(
            column_name="time_dominance",
            ascending=False,
            na_position="last",
            **(params_dict.get("sort_dominance_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=filter_dominance_values)
    )

    apply_dn_dom_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_dn_dom_colormap")
        .partial(
            input_column_name="time_dominance",
            colormap=["#a52a2a", "#ffd700"],
            output_column_name="time_dominance_colormap",
            **(params_dict.get("apply_dn_dom_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_dominance_values)
    )

    generate_td_ecomap_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_td_ecomap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "time_dominance_colormap",
                "opacity": 0.25,
                "stroked": True,
            },
            legend={
                "label_column": "time_dominance",
                "color_column": "time_dominance_colormap",
            },
            tooltip_columns=["density", "time_dominance"],
            **(params_dict.get("generate_td_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_dn_dom_colormap)
    )

    combine_landdx_td_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_td_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_landdx_td_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_td_ecomap_layers)
    )

    td_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="td_view_zip")
        .partial(
            left=combine_landdx_td_layers,
            right=zoom_traj_view,
            **(params_dict.get("td_view_zip") or {}),
        )
        .call()
    )

    draw_tdominance_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_tdominance_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={
                "placement": "bottom-right",
                "title": "Time of Day Dominance",
            },
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_tdominance_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=td_view_zip)
    )

    persist_tdominance_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_tdominance_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_tdominance_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_tdominance_ecomaps)
    )

    create_tdom_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_tdom_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Day/Night Ecomap",
            **(params_dict.get("create_tdom_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_tdominance_ecomap_urls)
    )

    merge_td_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_td_ecomap_widgets")
        .partial(
            widgets=create_tdom_ecomap_widgets,
            **(params_dict.get("merge_td_ecomap_widgets") or {}),
        )
        .call()
    )

    night_proportion_fixes = (
        compute_fix_density_and_night_class.validate()
        .handle_errors(task_instance_id="night_proportion_fixes")
        .partial(
            mode="proportion",
            grid=create_dn_meshgrid,
            geometry_type="point",
            threshold=0.65,
            **(params_dict.get("night_proportion_fixes") or {}),
        )
        .mapvalues(argnames=["selection"], argvalues=split_relocations_by_group)
    )

    sort_night_proportion_values = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_night_proportion_values")
        .partial(
            column_name="night_class",
            ascending=False,
            na_position="last",
            **(params_dict.get("sort_night_proportion_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=night_proportion_fixes)
    )

    drop_nan_proportion_values = (
        drop_nan_values_by_column.validate()
        .handle_errors(task_instance_id="drop_nan_proportion_values")
        .partial(
            column_name="density",
            **(params_dict.get("drop_nan_proportion_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_night_proportion_values)
    )

    drop_nan_threshold_values = (
        drop_missing_values_by_column.validate()
        .handle_errors(task_instance_id="drop_nan_threshold_values")
        .partial(
            column_name="night_class",
            **(params_dict.get("drop_nan_threshold_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_nan_proportion_values)
    )

    apply_nightpr_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_nightpr_colormap")
        .partial(
            input_column_name="night_class",
            colormap=["#6495ed", "#00008b"],
            output_column_name="night_class_colormap",
            **(params_dict.get("apply_nightpr_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=drop_nan_threshold_values)
    )

    generate_nightpr_ecomap_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_nightpr_ecomap_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "night_class_colormap",
                "opacity": 0.25,
                "stroked": True,
            },
            legend={
                "label_column": "night_class",
                "color_column": "night_class_colormap",
            },
            tooltip_columns=["density", "night_class", "night_class_colormap"],
            **(params_dict.get("generate_nightpr_ecomap_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_nightpr_colormap)
    )

    combine_landdx_np_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_landdx_np_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_landdx_np_layers") or {}),
        )
        .mapvalues(
            argnames=["grouped_layers"], argvalues=generate_nightpr_ecomap_layers
        )
    )

    np_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="np_view_zip")
        .partial(
            left=combine_landdx_np_layers,
            right=zoom_traj_view,
            **(params_dict.get("np_view_zip") or {}),
        )
        .call()
    )

    draw_nightpr_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_nightpr_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Night Time Fixes"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_nightpr_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=np_view_zip)
    )

    persist_nightpr_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_nightpr_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_nightpr_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_nightpr_ecomaps)
    )

    create_nightpr_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_nightpr_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Proportion of night time fixes",
            **(params_dict.get("create_nightpr_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_nightpr_ecomap_urls)
    )

    merge_npr_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_npr_ecomap_widgets")
        .partial(
            widgets=create_nightpr_ecomap_widgets,
            **(params_dict.get("merge_npr_ecomap_widgets") or {}),
        )
        .call()
    )

    filter_dry_trajs = (
        filter_column_values.validate()
        .handle_errors(task_instance_id="filter_dry_trajs")
        .partial(
            column="season",
            values=["dry"],
            **(params_dict.get("filter_dry_trajs") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_trajectories_by_group)
    )

    generate_dry_speed_rasters = (
        generate_ecograph_raster.validate()
        .handle_errors(task_instance_id="generate_dry_speed_rasters")
        .partial(
            dist_col="dist_meters",
            output_dir=create_output_directory,
            interpolation="mean",
            movement_covariate="speed",
            **(params_dict.get("generate_dry_speed_rasters") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=filter_dry_trajs)
    )

    extract_dry_speed_rasters = (
        retrieve_feature_gdf.validate()
        .handle_errors(task_instance_id="extract_dry_speed_rasters")
        .partial(**(params_dict.get("extract_dry_speed_rasters") or {}))
        .mapvalues(argnames=["file_path"], argvalues=generate_dry_speed_rasters)
    )

    sort_dry_speed_features = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_dry_speed_features")
        .partial(
            column_name="value",
            na_position="last",
            **(params_dict.get("sort_dry_speed_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=extract_dry_speed_rasters)
    )

    classify_dry_speed_features = (
        apply_classification.validate()
        .handle_errors(task_instance_id="classify_dry_speed_features")
        .partial(
            input_column_name="value",
            output_column_name="bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_dry_speed_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_dry_speed_features)
    )

    apply_dry_speed_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_dry_speed_colormap")
        .partial(
            input_column_name="bins",
            output_column_name="speedraster_bins_colors",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_dry_speed_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=classify_dry_speed_features)
    )

    format_dry_raster_labels = (
        map_values_with_unit.validate()
        .handle_errors(task_instance_id="format_dry_raster_labels")
        .partial(
            input_column_name="bins",
            output_column_name="bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_dry_raster_labels") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_dry_speed_colormap)
    )

    generate_dry_raster_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_dry_raster_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "speedraster_bins_colors",
                "opacity": 0.25,
                "stroked": True,
            },
            legend={
                "label_column": "bins_formatted",
                "color_column": "speedraster_bins_colors",
            },
            tooltip_columns=["value", "bins", "speedraster_bins_colors"],
            **(params_dict.get("generate_dry_raster_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=format_dry_raster_labels)
    )

    combine_dry_seasonal_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_dry_seasonal_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_dry_seasonal_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_dry_raster_layers)
    )

    dry_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="dry_view_zip")
        .partial(
            left=combine_dry_seasonal_layers,
            right=zoom_traj_view,
            **(params_dict.get("dry_view_zip") or {}),
        )
        .call()
    )

    draw_dry_speed_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_dry_speed_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Raster Value(Km/h)"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_dry_speed_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=dry_view_zip)
    )

    dry_raster_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="dry_raster_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("dry_raster_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_dry_speed_ecomaps)
    )

    dry_single_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="dry_single_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Dry Speed Raster Ecomap",
            **(params_dict.get("dry_single_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=dry_raster_ecomap_urls)
    )

    dry_raster_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="dry_raster_ecomap_widgets")
        .partial(
            widgets=dry_single_ecomap_widgets,
            **(params_dict.get("dry_raster_ecomap_widgets") or {}),
        )
        .call()
    )

    filter_wet_trajs = (
        filter_column_values.validate()
        .handle_errors(task_instance_id="filter_wet_trajs")
        .partial(
            column="season",
            values=["wet"],
            **(params_dict.get("filter_wet_trajs") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_trajectories_by_group)
    )

    generate_wet_speed_rasters = (
        generate_ecograph_raster.validate()
        .handle_errors(task_instance_id="generate_wet_speed_rasters")
        .partial(
            dist_col="dist_meters",
            output_dir=create_output_directory,
            interpolation="mean",
            movement_covariate="speed",
            **(params_dict.get("generate_wet_speed_rasters") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=filter_wet_trajs)
    )

    extract_wet_speed_rasters = (
        retrieve_feature_gdf.validate()
        .handle_errors(task_instance_id="extract_wet_speed_rasters")
        .partial(**(params_dict.get("extract_wet_speed_rasters") or {}))
        .mapvalues(argnames=["file_path"], argvalues=generate_wet_speed_rasters)
    )

    sort_wet_speed_features = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_wet_speed_features")
        .partial(
            column_name="value",
            na_position="last",
            **(params_dict.get("sort_wet_speed_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=extract_wet_speed_rasters)
    )

    classify_wet_speed_features = (
        apply_classification.validate()
        .handle_errors(task_instance_id="classify_wet_speed_features")
        .partial(
            input_column_name="value",
            output_column_name="bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_wet_speed_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_wet_speed_features)
    )

    apply_wet_speed_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_wet_speed_colormap")
        .partial(
            input_column_name="bins",
            output_column_name="speedraster_bins_colors",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_wet_speed_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=classify_wet_speed_features)
    )

    format_wet_raster_labels = (
        map_values_with_unit.validate()
        .handle_errors(task_instance_id="format_wet_raster_labels")
        .partial(
            input_column_name="bins",
            output_column_name="bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            **(params_dict.get("format_wet_raster_labels") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=apply_wet_speed_colormap)
    )

    generate_wet_raster_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_wet_raster_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "speedraster_bins_colors",
                "opacity": 0.25,
                "stroked": True,
            },
            legend={
                "label_column": "bins_formatted",
                "color_column": "speedraster_bins_colors",
            },
            tooltip_columns=["value", "bins", "speedraster_bins_colors"],
            **(params_dict.get("generate_wet_raster_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=format_wet_raster_labels)
    )

    combine_wet_seasonal_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_wet_seasonal_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_wet_seasonal_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_wet_raster_layers)
    )

    wets_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="wets_view_zip")
        .partial(
            left=combine_wet_seasonal_layers,
            right=zoom_traj_view,
            **(params_dict.get("wets_view_zip") or {}),
        )
        .call()
    )

    draw_wet_speed_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_wet_speed_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Raster Value(Km/h)"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_wet_speed_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=wets_view_zip)
    )

    wet_raster_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="wet_raster_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("wet_raster_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_wet_speed_ecomaps)
    )

    wet_single_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="wet_single_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Wet Speed Raster Ecomap",
            **(params_dict.get("wet_single_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=wet_raster_ecomap_urls)
    )

    wet_raster_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="wet_raster_ecomap_widgets")
        .partial(
            widgets=wet_single_ecomap_widgets,
            **(params_dict.get("wet_raster_ecomap_widgets") or {}),
        )
        .call()
    )

    generate_recursion_rasters = (
        generate_ecograph_raster.validate()
        .handle_errors(task_instance_id="generate_recursion_rasters")
        .partial(
            dist_col="dist_meters",
            output_dir=create_output_directory,
            interpolation="mean",
            network_metric="weight",
            **(params_dict.get("generate_recursion_rasters") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=split_trajectories_by_group)
    )

    extract_recursion_rasters = (
        retrieve_feature_gdf.validate()
        .handle_errors(task_instance_id="extract_recursion_rasters")
        .partial(**(params_dict.get("extract_recursion_rasters") or {}))
        .mapvalues(argnames=["file_path"], argvalues=generate_recursion_rasters)
    )

    sort_recursion_features = (
        sort_values.validate()
        .handle_errors(task_instance_id="sort_recursion_features")
        .partial(
            column_name="value",
            na_position="last",
            **(params_dict.get("sort_recursion_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=extract_recursion_rasters)
    )

    classify_recursion_features = (
        apply_classification.validate()
        .handle_errors(task_instance_id="classify_recursion_features")
        .partial(
            input_column_name="value",
            output_column_name="bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            **(params_dict.get("classify_recursion_features") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=sort_recursion_features)
    )

    apply_recursion_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="apply_recursion_colormap")
        .partial(
            input_column_name="bins",
            output_column_name="recursion_bins_colors",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            **(params_dict.get("apply_recursion_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=classify_recursion_features)
    )

    generate_recursion_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_recursion_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "recursion_bins_colors",
                "opacity": 0.25,
                "stroked": True,
            },
            legend={"label_column": "bins", "color_column": "recursion_bins_colors"},
            tooltip_columns=["value", "bins", "recursion_bins_colors"],
            **(params_dict.get("generate_recursion_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=apply_recursion_colormap)
    )

    combine_recursion_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_recursion_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_recursion_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_recursion_layers)
    )

    recursion_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="recursion_view_zip")
        .partial(
            left=combine_landdx_np_layers,
            right=zoom_traj_view,
            **(params_dict.get("recursion_view_zip") or {}),
        )
        .call()
    )

    draw_recursion_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_recursion_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Recursion Events"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_recursion_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=recursion_view_zip)
    )

    recursion_ecomap_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="recursion_ecomap_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("recursion_ecomap_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_recursion_ecomaps)
    )

    recursion_ecomap_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="recursion_ecomap_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Recursion Events Raster Ecomap",
            **(params_dict.get("recursion_ecomap_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=recursion_ecomap_urls)
    )

    reraster_ecomap_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="reraster_ecomap_widgets")
        .partial(
            widgets=recursion_ecomap_widgets,
            **(params_dict.get("reraster_ecomap_widgets") or {}),
        )
        .call()
    )

    filter_protected_trajs = (
        filter_column_values.validate()
        .handle_errors(task_instance_id="filter_protected_trajs")
        .partial(
            column="area_status",
            values=["protected"],
            **(params_dict.get("filter_protected_trajs") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_trajectories_by_group)
    )

    generate_protected_etd = (
        calculate_elliptical_time_density.validate()
        .handle_errors(task_instance_id="generate_protected_etd")
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.0],
            nodata_value="nan",
            band_count=1,
            **(params_dict.get("generate_protected_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=filter_protected_trajs)
    )

    td_protected_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="td_protected_colormap")
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("td_protected_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_protected_etd)
    )

    generate_etd_protected_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_etd_protected_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "percentile_colormap",
                "opacity": 0.25,
                "stroked": False,
            },
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("generate_etd_protected_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=td_protected_colormap)
    )

    combine_ldx_protected_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_ldx_protected_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_ldx_protected_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_etd_protected_layers)
    )

    prot_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="prot_view_zip")
        .partial(
            left=combine_ldx_protected_layers,
            right=zoom_traj_view,
            **(params_dict.get("prot_view_zip") or {}),
        )
        .call()
    )

    draw_protected_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_protected_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Home Range Metrics"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_protected_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=prot_view_zip)
    )

    persist_protected_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_protected_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_protected_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_protected_ecomaps)
    )

    create_protected_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_protected_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Protected Home Range Ecomap",
            **(params_dict.get("create_protected_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_protected_urls)
    )

    merge_protected_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_protected_widgets")
        .partial(
            widgets=create_protected_widgets,
            **(params_dict.get("merge_protected_widgets") or {}),
        )
        .call()
    )

    filter_unprotected_trajs = (
        filter_column_values.validate()
        .handle_errors(task_instance_id="filter_unprotected_trajs")
        .partial(
            column="area_status",
            values=["unprotected"],
            **(params_dict.get("filter_unprotected_trajs") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_trajectories_by_group)
    )

    generate_unprotected_etd = (
        calculate_elliptical_time_density.validate()
        .handle_errors(task_instance_id="generate_unprotected_etd")
        .partial(
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.0],
            nodata_value="nan",
            band_count=1,
            **(params_dict.get("generate_unprotected_etd") or {}),
        )
        .mapvalues(argnames=["trajectory_gdf"], argvalues=filter_unprotected_trajs)
    )

    td_unprotected_colormap = (
        apply_color_map.validate()
        .handle_errors(task_instance_id="td_unprotected_colormap")
        .partial(
            input_column_name="percentile",
            colormap="RdYlGn",
            output_column_name="percentile_colormap",
            **(params_dict.get("td_unprotected_colormap") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=generate_unprotected_etd)
    )

    generate_unprotected_layers = (
        create_polygon_layer.validate()
        .handle_errors(task_instance_id="generate_unprotected_layers")
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "fill_color_column": "percentile_colormap",
                "opacity": 0.25,
                "stroked": False,
            },
            legend={
                "label_column": "percentile",
                "color_column": "percentile_colormap",
            },
            tooltip_columns=["percentile"],
            **(params_dict.get("generate_unprotected_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=td_unprotected_colormap)
    )

    combine_ldx_unprotected_layers = (
        combine_map_layers.validate()
        .handle_errors(task_instance_id="combine_ldx_unprotected_layers")
        .partial(
            static_layers=create_styled_landdx_layers,
            **(params_dict.get("combine_ldx_unprotected_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_unprotected_layers)
    )

    unprot_view_zip = (
        zip_grouped_by_key.validate()
        .handle_errors(task_instance_id="unprot_view_zip")
        .partial(
            left=combine_landdx_np_layers,
            right=zoom_traj_view,
            **(params_dict.get("unprot_view_zip") or {}),
        )
        .call()
    )

    draw_unprotected_ecomaps = (
        draw_ecomap.validate()
        .handle_errors(task_instance_id="draw_unprotected_ecomaps")
        .partial(
            tile_layers=configure_base_maps,
            north_arrow_style={"placement": "top-left"},
            legend_style={"placement": "bottom-right", "title": "Home Range Metrics"},
            static=False,
            title=None,
            max_zoom=20,
            **(params_dict.get("draw_unprotected_ecomaps") or {}),
        )
        .mapvalues(argnames=["geo_layers", "view_state"], argvalues=unprot_view_zip)
    )

    persist_unprotected_urls = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_unprotected_urls")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_unprotected_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=draw_unprotected_ecomaps)
    )

    create_unprotected_widgets = (
        create_map_widget_single_view.validate()
        .handle_errors(task_instance_id="create_unprotected_widgets")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Unprotected Home Range Ecomap",
            **(params_dict.get("create_unprotected_widgets") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_unprotected_urls)
    )

    merge_unprotected_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_unprotected_widgets")
        .partial(
            widgets=create_unprotected_widgets,
            **(params_dict.get("merge_unprotected_widgets") or {}),
        )
        .call()
    )

    unpr_category_summary = (
        category_summary.validate()
        .handle_errors(task_instance_id="unpr_category_summary")
        .partial(
            group_cols=["extra__name", "season"],
            category_col="area_status",
            pct_for=["protected", "unprotected"],
            **(params_dict.get("unpr_category_summary") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_trajectories_by_group)
    )

    plot_prot_unprot = (
        plot_protected_fix_proportions.validate()
        .handle_errors(task_instance_id="plot_prot_unprot")
        .partial(title="proportion", **(params_dict.get("plot_prot_unprot") or {}))
        .mapvalues(argnames=["summary"], argvalues=unpr_category_summary)
    )

    persist_pr_chart = (
        persist_text.validate()
        .handle_errors(task_instance_id="persist_pr_chart")
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_pr_chart") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=plot_prot_unprot)
    )

    create_single_prot_widget = (
        create_plot_widget_single_view.validate()
        .handle_errors(task_instance_id="create_single_prot_widget")
        .skipif(
            conditions=[
                never,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Proportion of Fixes",
            **(params_dict.get("create_single_prot_widget") or {}),
        )
        .map(argnames=["view", "data"], argvalues=persist_pr_chart)
    )

    merge_prot_widgets = (
        merge_widget_views.validate()
        .handle_errors(task_instance_id="merge_prot_widgets")
        .partial(
            widgets=create_single_prot_widget,
            **(params_dict.get("merge_prot_widgets") or {}),
        )
        .call()
    )

    mapbook_dashboard = (
        gather_dashboard.validate()
        .handle_errors(task_instance_id="mapbook_dashboard")
        .partial(
            details=initialize_workflow_metadata,
            widgets=[
                merge_substyled_widgets,
                merge_hr_ecomap_widgets,
                merge_custom_hr_ecomap_widgets,
                merge_season_ecomap_widgets,
                merge_td_ecomap_widgets,
                merge_npr_ecomap_widgets,
                dry_raster_ecomap_widgets,
                wet_raster_ecomap_widgets,
                reraster_ecomap_widgets,
                merge_protected_widgets,
                merge_unprotected_widgets,
                merge_prot_widgets,
            ],
            time_range=define_time_range,
            groupers=configure_grouping_strategy,
            **(params_dict.get("mapbook_dashboard") or {}),
        )
        .call()
    )

    return mapbook_dashboard
