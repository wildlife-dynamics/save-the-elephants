# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection as set_gee_connection
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import filter_df as filter_df
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_scatterplot_layer as create_scatterplot_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    filter_row_values as filter_row_values,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    create_meshgrid as create_meshgrid,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import get_events as get_events
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    classify_is_night as classify_is_night,
)
from ecoscope_workflows_ext_ste.tasks import (
    annotate_gdf_dict_with_geom_type as annotate_gdf_dict_with_geom_type,
)
from ecoscope_workflows_ext_ste.tasks import (
    assign_season_colors as assign_season_colors,
)
from ecoscope_workflows_ext_ste.tasks import (
    calculate_seasonal_home_range as calculate_seasonal_home_range,
)
from ecoscope_workflows_ext_ste.tasks import (
    combine_deckgl_map_layers as combine_deckgl_map_layers,
)
from ecoscope_workflows_ext_ste.tasks import convert_hex_to_rgba as convert_hex_to_rgba
from ecoscope_workflows_ext_ste.tasks import convert_to_str as convert_to_str
from ecoscope_workflows_ext_ste.tasks import create_column as create_column
from ecoscope_workflows_ext_ste.tasks import create_context_page as create_context_page
from ecoscope_workflows_ext_ste.tasks import (
    create_custom_text_layer as create_custom_text_layer,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_deckgl_layers_from_gdf_dict as create_deckgl_layers_from_gdf_dict,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_mapbook_ctx_cover as create_mapbook_ctx_cover,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_seasonal_labels as create_seasonal_labels,
)
from ecoscope_workflows_ext_ste.tasks import (
    custom_determine_season_windows as custom_determine_season_windows,
)
from ecoscope_workflows_ext_ste.tasks import (
    custom_trajectory_segment_filter as custom_trajectory_segment_filter,
)
from ecoscope_workflows_ext_ste.tasks import (
    dataframe_column_first_unique_str as dataframe_column_first_unique_str,
)
from ecoscope_workflows_ext_ste.tasks import (
    determine_previous_period as determine_previous_period,
)
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import filter_df_cols as filter_df_cols
from ecoscope_workflows_ext_ste.tasks import filter_df_values as filter_df_values
from ecoscope_workflows_ext_ste.tasks import (
    general_template_context as general_template_context,
)
from ecoscope_workflows_ext_ste.tasks import (
    generate_ecograph_raster as generate_ecograph_raster,
)
from ecoscope_workflows_ext_ste.tasks import (
    generate_protected_column as generate_protected_column,
)
from ecoscope_workflows_ext_ste.tasks import (
    get_day_night_dominance as get_day_night_dominance,
)
from ecoscope_workflows_ext_ste.tasks import get_file_path as get_file_path
from ecoscope_workflows_ext_ste.tasks import (
    get_grid_night_fixes as get_grid_night_fixes,
)
from ecoscope_workflows_ext_ste.tasks import merge_mapbook_files as merge_mapbook_files
from ecoscope_workflows_ext_ste.tasks import merge_multiple_df as merge_multiple_df
from ecoscope_workflows_ext_ste.tasks import (
    modify_status_colors as modify_status_colors,
)
from ecoscope_workflows_ext_ste.tasks import (
    plot_fix_protection_status as plot_fix_protection_status,
)
from ecoscope_workflows_ext_ste.tasks import (
    retrieve_feature_gdf as retrieve_feature_gdf,
)
from ecoscope_workflows_ext_ste.tasks import split_gdf_by_column as split_gdf_by_column
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf
from ecoscope_workflows_ext_ste.tasks import zip_groupbykey as zip_groupbykey

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("time_range") or {}))
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupers=[{"index_name": "subject_name"}],
            **(params_dict.get("groupers") or {}),
        )
        .call()
    )

    configure_base_maps = (
        set_base_maps_pydeck.validate()
        .set_task_instance_id("configure_base_maps")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            base_maps=[
                {
                    "url": "https://server.arcgisonline.com/arcgis/rest/services/Elevation/World_Hillshade/MapServer/tile/{z}/{y}/{x}",
                    "opacity": 1,
                    "max_zoom": 20,
                },
                {
                    "url": "https://services.arcgisonline.com/ArcGIS/rest/services/Reference/World_Boundaries_and_Places_Alternate/MapServer/tile/{z}/{y}/{x}",
                    "opacity": 1,
                    "max_zoom": 20,
                },
            ],
            **(params_dict.get("configure_base_maps") or {}),
        )
        .call()
    )

    set_previous_period = (
        determine_previous_period.validate()
        .set_task_instance_id("set_previous_period")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            current_time_range=time_range,
            **(params_dict.get("set_previous_period") or {}),
        )
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    gee_project_name = (
        set_gee_connection.validate()
        .set_task_instance_id("gee_project_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("gee_project_name") or {}))
        .call()
    )

    subject_group_var = (
        set_string_var.validate()
        .set_task_instance_id("subject_group_var")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("subject_group_var") or {}))
        .call()
    )

    subject_observations = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_observations")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filter="clean",
            client=er_client_name,
            time_range=time_range,
            subject_group_name=subject_group_var,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("subject_observations") or {}),
        )
        .call()
    )

    retrieve_ldx_db = (
        get_file_path.validate()
        .set_task_instance_id("retrieve_ldx_db")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("retrieve_ldx_db") or {}),
        )
        .call()
    )

    load_ldx = (
        load_df.validate()
        .set_task_instance_id("load_ldx")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=retrieve_ldx_db,
            layer="landDx_polygons",
            deserialize_json=False,
            **(params_dict.get("load_ldx") or {}),
        )
        .call()
    )

    filter_ldx_aoi = (
        filter_row_values.validate()
        .set_task_instance_id("filter_ldx_aoi")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=load_ldx,
            column="type",
            values=["Community Conservancy", "National Reserve", "National Park"],
            **(params_dict.get("filter_ldx_aoi") or {}),
        )
        .call()
    )

    filter_ldx_cols = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_ldx_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_ldx_aoi,
            columns=["type", "name", "geometry"],
            **(params_dict.get("filter_ldx_cols") or {}),
        )
        .call()
    )

    create_ldx_text_layer = (
        create_custom_text_layer.validate()
        .set_task_instance_id("create_ldx_text_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            geodataframe=filter_ldx_cols,
            layer_style={
                "get_text": "name",
                "get_color": [20, 20, 20, 255],
                "get_size": 1000,
                "size_units": "meters",
                "size_min_pixels": 40,
                "size_max_pixels": 75,
                "size_scale": 1.25,
                "font_family": "Arial",
                "font_weight": "normal",
                "get_text_anchor": "middle",
                "get_alignment_baseline": "center",
                "billboard": True,
                "background_padding": [4, 8],
                "pickable": True,
                "auto_highlight": False,
            },
            use_centroid=True,
            legend=None,
            **(params_dict.get("create_ldx_text_layer") or {}),
        )
        .call()
    )

    split_ldx_by_type = (
        split_gdf_by_column.validate()
        .set_task_instance_id("split_ldx_by_type")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=filter_ldx_cols,
            column="type",
            **(params_dict.get("split_ldx_by_type") or {}),
        )
        .call()
    )

    annotate_gdf_dict = (
        annotate_gdf_dict_with_geom_type.validate()
        .set_task_instance_id("annotate_gdf_dict")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf_dict=split_ldx_by_type, **(params_dict.get("annotate_gdf_dict") or {})
        )
        .call()
    )

    create_ldx_styled_layers = (
        create_deckgl_layers_from_gdf_dict.validate()
        .set_task_instance_id("create_ldx_styled_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf_dict=annotate_gdf_dict,
            styles={
                "Community Conservancy": {
                    "get_fill_color": [166, 182, 151],
                    "get_line_color": [166, 182, 151],
                    "opacity": 0.175,
                    "stroked": True,
                    "get_line_width": 2.25,
                },
                "National Reserve": {
                    "get_fill_color": [136, 167, 142],
                    "get_line_color": [136, 167, 142],
                    "opacity": 0.175,
                    "stroked": True,
                    "get_line_width": 2.25,
                },
                "National Park": {
                    "get_fill_color": [17, 86, 49],
                    "get_line_color": [17, 86, 49],
                    "opacity": 0.175,
                    "stroked": True,
                    "get_line_width": 2.25,
                },
            },
            legends={
                "title": "Land Use",
                "values": [
                    {"label": "Community Conservancy", "color": "#a6b697"},
                    {"label": "National Reserve", "color": "#88a78e"},
                    {"label": "National Park", "color": "#115631"},
                ],
            },
            **(params_dict.get("create_ldx_styled_layers") or {}),
        )
        .call()
    )

    subject_reloc = (
        process_relocations.validate()
        .set_task_instance_id("subject_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=subject_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__hex",
                "extra__subject__sex",
                "extra__created_at",
                "extra__subject__subject_subtype",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("subject_reloc") or {}),
        )
        .call()
    )

    annotate_day_night = (
        classify_is_night.validate()
        .set_task_instance_id("annotate_day_night")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=subject_reloc, **(params_dict.get("annotate_day_night") or {})
        )
        .call()
    )

    custom_trajs_filter = (
        custom_trajectory_segment_filter.validate()
        .set_task_instance_id("custom_trajs_filter")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("custom_trajs_filter") or {}))
        .call()
    )

    convert_to_trajectories = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("convert_to_trajectories")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=annotate_day_night,
            trajectory_segment_filter=custom_trajs_filter,
            **(params_dict.get("convert_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_index_to_traj = (
        add_temporal_index.validate()
        .set_task_instance_id("add_temporal_index_to_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_to_trajectories,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_index_to_traj") or {}),
        )
        .call()
    )

    previous_observations = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("previous_observations")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            filter="clean",
            client=er_client_name,
            time_range=set_previous_period,
            subject_group_name=subject_group_var,
            raise_on_empty=False,
            include_details=False,
            include_subjectsource_details=False,
            **(params_dict.get("previous_observations") or {}),
        )
        .call()
    )

    previous_relocs = (
        process_relocations.validate()
        .set_task_instance_id("previous_relocs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=previous_observations,
            relocs_columns=[
                "groupby_col",
                "fixtime",
                "junk_status",
                "geometry",
                "extra__subject__name",
                "extra__subject__hex",
                "extra__subject__sex",
                "extra__created_at",
                "extra__subject__subject_subtype",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("previous_relocs") or {}),
        )
        .call()
    )

    annotate_prev_day_night = (
        classify_is_night.validate()
        .set_task_instance_id("annotate_prev_day_night")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=previous_relocs,
            **(params_dict.get("annotate_prev_day_night") or {}),
        )
        .call()
    )

    convert_prev_to_trajectories = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("convert_prev_to_trajectories")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            relocations=annotate_prev_day_night,
            trajectory_segment_filter=custom_trajs_filter,
            **(params_dict.get("convert_prev_to_trajectories") or {}),
        )
        .call()
    )

    add_temporal_prev_index_to_traj = (
        add_temporal_index.validate()
        .set_task_instance_id("add_temporal_prev_index_to_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=convert_prev_to_trajectories,
            time_col="segment_start",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("add_temporal_prev_index_to_traj") or {}),
        )
        .call()
    )

    rename_prev_traj_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_prev_traj_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            raise_if_not_found=True,
            df=add_temporal_prev_index_to_traj,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__hex": "hex_color",
                "extra__is_night": "is_night",
                "extra__name": "subject_name",
                "extra__sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
                "extra__created_at": "created_at",
            },
            **(params_dict.get("rename_prev_traj_cols") or {}),
        )
        .call()
    )

    classify_trajectories_speed_bins = (
        apply_classification.validate()
        .set_task_instance_id("classify_trajectories_speed_bins")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_temporal_index_to_traj,
            input_column_name="speed_kmhr",
            output_column_name="speed_bins",
            classification_options={"scheme": "equal_interval", "k": 6},
            label_options={
                "label_ranges": True,
                "label_decimals": 1,
                "label_suffix": " km/h",
            },
            **(params_dict.get("classify_trajectories_speed_bins") or {}),
        )
        .call()
    )

    rename_traj_cols = (
        map_columns.validate()
        .set_task_instance_id("rename_traj_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            raise_if_not_found=True,
            df=classify_trajectories_speed_bins,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__hex": "hex_color",
                "extra__is_night": "is_night",
                "extra__name": "subject_name",
                "extra__sex": "subject_sex",
                "extra__subject_subtype": "subject_subtype",
                "extra__created_at": "created_at",
            },
            **(params_dict.get("rename_traj_cols") or {}),
        )
        .call()
    )

    persist_prev_trajs_geoparquet = (
        persist_df.validate()
        .set_task_instance_id("persist_prev_trajs_geoparquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_prev_traj_cols,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="previous_period_trajectories",
            **(params_dict.get("persist_prev_trajs_geoparquet") or {}),
        )
        .call()
    )

    persist_relocs_geoparquet = (
        persist_df.validate()
        .set_task_instance_id("persist_relocs_geoparquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=annotate_day_night,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="relocations",
            **(params_dict.get("persist_relocs_geoparquet") or {}),
        )
        .call()
    )

    persist_prev_relocs_geoparquet = (
        persist_df.validate()
        .set_task_instance_id("persist_prev_relocs_geoparquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=previous_relocs,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="previous_period_relocations",
            **(params_dict.get("persist_prev_relocs_geoparquet") or {}),
        )
        .call()
    )

    create_current_duration_column = (
        create_column.validate()
        .set_task_instance_id("create_current_duration_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            col_name="duration_status",
            value="Current tracks",
            **(params_dict.get("create_current_duration_column") or {}),
        )
        .call()
    )

    create_previous_duration_column = (
        create_column.validate()
        .set_task_instance_id("create_previous_duration_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_prev_traj_cols,
            col_name="duration_status",
            value="Previous tracks",
            **(params_dict.get("create_previous_duration_column") or {}),
        )
        .call()
    )

    merge_current_prev_trajs = (
        merge_multiple_df.validate()
        .set_task_instance_id("merge_current_prev_trajs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            list_df=[create_current_duration_column, create_previous_duration_column],
            ignore_index=True,
            sort=False,
            **(params_dict.get("merge_current_prev_trajs") or {}),
        )
        .call()
    )

    assign_duration_colors = (
        modify_status_colors.validate()
        .set_task_instance_id("assign_duration_colors")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            grouper_value="overall",
            gdf=merge_current_prev_trajs,
            **(params_dict.get("assign_duration_colors") or {}),
        )
        .call()
    )

    sort_trajs_by_status = (
        sort_values.validate()
        .set_task_instance_id("sort_trajs_by_status")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="duration_status",
            na_position="first",
            ascending=False,
            df=assign_duration_colors,
            **(params_dict.get("sort_trajs_by_status") or {}),
        )
        .call()
    )

    filter_movement_cols = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_movement_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=[
                "duration_status",
                "duration_status_colors",
                "is_night",
                "geometry",
            ],
            df=sort_trajs_by_status,
            **(params_dict.get("filter_movement_cols") or {}),
        )
        .call()
    )

    generate_track_layers = (
        create_path_layer.validate()
        .set_task_instance_id("generate_track_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "duration_status_colors",
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Movement Tracks",
                "label_column": "duration_status",
                "color_column": "duration_status_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=filter_movement_cols,
            **(params_dict.get("generate_track_layers") or {}),
        )
        .call()
    )

    combined_ldx_movement_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_movement_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_track_layers,
            **(params_dict.get("combined_ldx_movement_layers") or {}),
        )
        .call()
    )

    movement_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("movement_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=filter_movement_cols,
            pitch=0,
            bearing=0,
            **(params_dict.get("movement_view_state") or {}),
        )
        .call()
    )

    draw_movement_tracks = (
        draw_map.validate()
        .set_task_instance_id("draw_movement_tracks")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            view_state=movement_view_state,
            geo_layers=combined_ldx_movement_layers,
            **(params_dict.get("draw_movement_tracks") or {}),
        )
        .call()
    )

    persist_movement_tracks_html = (
        persist_text.validate()
        .set_task_instance_id("persist_movement_tracks_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="movement_tracks.html",
            text=draw_movement_tracks,
            **(params_dict.get("persist_movement_tracks_html") or {}),
        )
        .call()
    )

    get_events_data = (
        get_events.validate()
        .set_task_instance_id("get_events_data")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            event_columns=[
                "id",
                "time",
                "event_type",
                "event_category",
                "reported_by",
                "serial_number",
                "geometry",
                "created_at",
                "event_details",
            ],
            raise_on_empty=False,
            include_details=True,
            include_updates=False,
            include_related_events=False,
            include_null_geometry=False,
            include_display_values=False,
            **(params_dict.get("get_events_data") or {}),
        )
        .call()
    )

    generate_collaring_layers = (
        create_scatterplot_layer.validate()
        .set_task_instance_id("generate_collaring_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_fill_color": [85, 107, 47],
                "get_line_color": [0, 0, 0, 200],
                "get_line_width": 0.55,
                "get_radius": 3.55,
                "opacity": 0.75,
                "stroked": True,
            },
            legend={
                "title": "Legend",
                "values": [{"label": "Elephant sightings", "color": "#556b2f"}],
            },
            geodataframe=get_events_data,
            **(params_dict.get("generate_collaring_layers") or {}),
        )
        .call()
    )

    combined_ldx_collar_points = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_collar_points")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_collaring_layers,
            **(params_dict.get("combined_ldx_collar_points") or {}),
        )
        .call()
    )

    collaring_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("collaring_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=get_events_data,
            pitch=0,
            bearing=0,
            **(params_dict.get("collaring_view_state") or {}),
        )
        .call()
    )

    draw_collared_map = (
        draw_map.validate()
        .set_task_instance_id("draw_collared_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            view_state=collaring_view_state,
            geo_layers=combined_ldx_collar_points,
            **(params_dict.get("draw_collared_map") or {}),
        )
        .call()
    )

    persist_collared_points_html = (
        persist_text.validate()
        .set_task_instance_id("persist_collared_points_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="collared_points.html",
            text=draw_collared_map,
            **(params_dict.get("persist_collared_points_html") or {}),
        )
        .call()
    )

    convert_subject_hex_rgba = (
        convert_hex_to_rgba.validate()
        .set_task_instance_id("convert_subject_hex_rgba")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=rename_traj_cols,
            col="hex_color",
            new_col="rgba_colors",
            **(params_dict.get("convert_subject_hex_rgba") or {}),
        )
        .call()
    )

    generate_strack_layers = (
        create_path_layer.validate()
        .set_task_instance_id("generate_strack_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_color": "rgba_colors",
                "get_width": 2.85,
                "width_scale": 1,
                "width_min_pixels": 2,
                "width_max_pixels": 8,
                "width_units": "pixels",
                "cap_rounded": True,
                "joint_rounded": True,
                "billboard": False,
                "opacity": 0.55,
                "stroked": True,
            },
            legend={
                "title": "Movement Tracks",
                "label_column": "subject_name",
                "color_column": "rgba_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=convert_subject_hex_rgba,
            **(params_dict.get("generate_strack_layers") or {}),
        )
        .call()
    )

    combined_ldx_subject_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_subject_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_strack_layers,
            **(params_dict.get("combined_ldx_subject_layers") or {}),
        )
        .call()
    )

    tracks_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("tracks_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=convert_subject_hex_rgba,
            pitch=0,
            bearing=0,
            **(params_dict.get("tracks_view_state") or {}),
        )
        .call()
    )

    draw_subject_tracks = (
        draw_map.validate()
        .set_task_instance_id("draw_subject_tracks")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            view_state=tracks_view_state,
            geo_layers=combined_ldx_subject_layers,
            **(params_dict.get("draw_subject_tracks") or {}),
        )
        .call()
    )

    persist_subject_tracks_html = (
        persist_text.validate()
        .set_task_instance_id("persist_subject_tracks_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="subject_tracks.html",
            text=draw_subject_tracks,
            **(params_dict.get("persist_subject_tracks_html") or {}),
        )
        .call()
    )

    generate_etd = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("generate_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            auto_scale_or_custom_cell_size={"auto_scale_or_customize": "Auto-scale"},
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
            nodata_value="nan",
            band_count=1,
            max_speed_factor=1.05,
            expansion_factor=1.3,
            trajectory_gdf=rename_traj_cols,
            **(params_dict.get("generate_etd") or {}),
        )
        .call()
    )

    persist_etd_gdf = (
        persist_df.validate()
        .set_task_instance_id("persist_etd_gdf")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="home_range_etd",
            df=generate_etd,
            **(params_dict.get("persist_etd_gdf") or {}),
        )
        .call()
    )

    determine_seasonal_windows = (
        custom_determine_season_windows.validate()
        .set_task_instance_id("determine_seasonal_windows")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=gee_project_name,
            time_range=time_range,
            roi=generate_etd,
            **(params_dict.get("determine_seasonal_windows") or {}),
        )
        .call()
    )

    persist_ndvi_values = (
        persist_df.validate()
        .set_task_instance_id("persist_ndvi_values")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="csv",
            filename="seasonal_windows",
            df=determine_seasonal_windows,
            **(params_dict.get("persist_ndvi_values") or {}),
        )
        .call()
    )

    add_season_labels = (
        create_seasonal_labels.validate()
        .set_task_instance_id("add_season_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            seasons_df=determine_seasonal_windows,
            trajectories=rename_traj_cols,
            **(params_dict.get("add_season_labels") or {}),
        )
        .call()
    )

    apply_etd_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_etd_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            output_column_name="etd_percentile_colors",
            colormap="RdYlGn",
            df=generate_etd,
            **(params_dict.get("apply_etd_colormap") or {}),
        )
        .call()
    )

    generate_home_range_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_home_range_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "etd_percentile_colors",
                "get_line_color": "etd_percentile_colors",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Home Range Percentiles",
                "label_column": "percentile",
                "color_column": "etd_percentile_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=apply_etd_colormap,
            **(params_dict.get("generate_home_range_layers") or {}),
        )
        .call()
    )

    combined_ldx_home_range_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_home_range_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_home_range_layers,
            **(params_dict.get("combined_ldx_home_range_layers") or {}),
        )
        .call()
    )

    overall_hr_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("overall_hr_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=apply_etd_colormap,
            pitch=0,
            bearing=0,
            **(params_dict.get("overall_hr_view_state") or {}),
        )
        .call()
    )

    draw_home_range_map = (
        draw_map.validate()
        .set_task_instance_id("draw_home_range_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_home_range_layers,
            view_state=overall_hr_view_state,
            **(params_dict.get("draw_home_range_map") or {}),
        )
        .call()
    )

    persist_homerange_html = (
        persist_text.validate()
        .set_task_instance_id("persist_homerange_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="overall_homerange.html",
            text=draw_home_range_map,
            **(params_dict.get("persist_homerange_html") or {}),
        )
        .call()
    )

    filter_percentile = (
        filter_df_values.validate()
        .set_task_instance_id("filter_percentile")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="percentile",
            op="ge",
            value=99.0,
            df=generate_etd,
            reset_index=True,
            **(params_dict.get("filter_percentile") or {}),
        )
        .call()
    )

    custom_home_range_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("custom_home_range_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": [210, 105, 30],
                "get_line_color": [210, 105, 30],
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Home Range Percentiles",
                "values": [{"label": "99th percentile", "color": "#d2691e"}],
            },
            geodataframe=filter_percentile,
            **(params_dict.get("custom_home_range_layers") or {}),
        )
        .call()
    )

    combined_ldx_filtered_hr_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_filtered_hr_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=custom_home_range_layers,
            **(params_dict.get("combined_ldx_filtered_hr_layers") or {}),
        )
        .call()
    )

    filtered_hr_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("filtered_hr_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=filter_percentile,
            pitch=0,
            bearing=0,
            **(params_dict.get("filtered_hr_view_state") or {}),
        )
        .call()
    )

    draw_filtered_hr_map = (
        draw_map.validate()
        .set_task_instance_id("draw_filtered_hr_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_filtered_hr_layers,
            view_state=filtered_hr_view_state,
            **(params_dict.get("draw_filtered_hr_map") or {}),
        )
        .call()
    )

    persist_filtered_hr_html = (
        persist_text.validate()
        .set_task_instance_id("persist_filtered_hr_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="filtered_homerange.html",
            text=draw_filtered_hr_map,
            **(params_dict.get("persist_filtered_hr_html") or {}),
        )
        .call()
    )

    generate_recursion_raster = (
        generate_ecograph_raster.validate()
        .set_task_instance_id("generate_recursion_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            step_length=2000,
            dist_col="dist_meters",
            interpolation="mean",
            movement_covariate=None,
            radius=2,
            cutoff=None,
            tortuosity_length=3,
            resolution=None,
            network_metric="weight",
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="weighted_raster",
            gdf=add_season_labels,
            **(params_dict.get("generate_recursion_raster") or {}),
        )
        .call()
    )

    extract_raster = (
        retrieve_feature_gdf.validate()
        .set_task_instance_id("extract_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=generate_recursion_raster,
            **(params_dict.get("extract_raster") or {}),
        )
        .call()
    )

    sort_recursion_features = (
        sort_values.validate()
        .set_task_instance_id("sort_recursion_features")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="value",
            na_position="last",
            ascending=True,
            df=extract_raster,
            **(params_dict.get("sort_recursion_features") or {}),
        )
        .call()
    )

    classify_recursion_features = (
        apply_classification.validate()
        .set_task_instance_id("classify_recursion_features")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value",
            output_column_name="bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_ranges": False, "label_decimals": 1},
            df=sort_recursion_features,
            **(params_dict.get("classify_recursion_features") or {}),
        )
        .call()
    )

    apply_recursion_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_recursion_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="bins",
            output_column_name="recursion_bins_colors",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            df=classify_recursion_features,
            **(params_dict.get("apply_recursion_colormap") or {}),
        )
        .call()
    )

    generate_recursion_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_recursion_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "recursion_bins_colors",
                "get_line_color": "recursion_bins_colors",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Recursion events",
                "label_column": "bins",
                "color_column": "recursion_bins_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=apply_recursion_colormap,
            **(params_dict.get("generate_recursion_layers") or {}),
        )
        .call()
    )

    combined_ldx_recursion_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_recursion_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_recursion_layers,
            **(params_dict.get("combined_ldx_recursion_layers") or {}),
        )
        .call()
    )

    recursion_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("recursion_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=apply_recursion_colormap,
            pitch=0,
            bearing=0,
            **(params_dict.get("recursion_view_state") or {}),
        )
        .call()
    )

    draw_recursion_map = (
        draw_map.validate()
        .set_task_instance_id("draw_recursion_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_recursion_layers,
            view_state=recursion_view_state,
            **(params_dict.get("draw_recursion_map") or {}),
        )
        .call()
    )

    persist_recursion_html = (
        persist_text.validate()
        .set_task_instance_id("persist_recursion_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="recursion_events.html",
            text=draw_recursion_map,
            **(params_dict.get("persist_recursion_html") or {}),
        )
        .call()
    )

    persist_trajs_geoparquet = (
        persist_df.validate()
        .set_task_instance_id("persist_trajs_geoparquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_season_labels,
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="trajectories",
            **(params_dict.get("persist_trajs_geoparquet") or {}),
        )
        .call()
    )

    filter_dry_df = (
        filter_df.validate()
        .set_task_instance_id("filter_dry_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="season",
            op="equal",
            value="dry",
            df=add_season_labels,
            reset_index=False,
            **(params_dict.get("filter_dry_df") or {}),
        )
        .call()
    )

    generate_dry_speed_raster = (
        generate_ecograph_raster.validate()
        .set_task_instance_id("generate_dry_speed_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            step_length=500,
            dist_col="dist_meters",
            interpolation="mean",
            movement_covariate="speed",
            radius=2,
            cutoff=None,
            tortuosity_length=3,
            resolution=None,
            network_metric=None,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename=None,
            gdf=filter_dry_df,
            **(params_dict.get("generate_dry_speed_raster") or {}),
        )
        .call()
    )

    extract_dry_rasters = (
        retrieve_feature_gdf.validate()
        .set_task_instance_id("extract_dry_rasters")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=generate_dry_speed_raster,
            **(params_dict.get("extract_dry_rasters") or {}),
        )
        .call()
    )

    sort_dry_speed_features = (
        sort_values.validate()
        .set_task_instance_id("sort_dry_speed_features")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="value",
            na_position="last",
            ascending=True,
            df=extract_dry_rasters,
            **(params_dict.get("sort_dry_speed_features") or {}),
        )
        .call()
    )

    apply_classification_dry = (
        apply_classification.validate()
        .set_task_instance_id("apply_classification_dry")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value",
            output_column_name="value_bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_range": False, "label_decimals": 1},
            df=sort_dry_speed_features,
            **(params_dict.get("apply_classification_dry") or {}),
        )
        .call()
    )

    apply_dry_raster_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_dry_raster_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value_bins",
            output_column_name="speedraster_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            df=apply_classification_dry,
            **(params_dict.get("apply_dry_raster_colormap") or {}),
        )
        .call()
    )

    format_dry_raster_labels = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_dry_raster_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value_bins",
            output_column_name="bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            df=apply_dry_raster_colormap,
            **(params_dict.get("format_dry_raster_labels") or {}),
        )
        .call()
    )

    create_dry_speed_raster_layer = (
        create_geojson_layer.validate()
        .set_task_instance_id("create_dry_speed_raster_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": False,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "speedraster_bins_colormap",
                "get_line_color": "speedraster_bins_colormap",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Mean Speed Raster (km/h)",
                "label_column": "bins_formatted",
                "color_column": "speedraster_bins_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=format_dry_raster_labels,
            **(params_dict.get("create_dry_speed_raster_layer") or {}),
        )
        .call()
    )

    combined_dry_speed_raster = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_dry_speed_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=create_dry_speed_raster_layer,
            **(params_dict.get("combined_dry_speed_raster") or {}),
        )
        .call()
    )

    dry_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("dry_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=format_dry_raster_labels,
            pitch=0,
            bearing=0,
            **(params_dict.get("dry_view_state") or {}),
        )
        .call()
    )

    draw_dry_raster_map = (
        draw_map.validate()
        .set_task_instance_id("draw_dry_raster_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_dry_speed_raster,
            view_state=dry_view_state,
            **(params_dict.get("draw_dry_raster_map") or {}),
        )
        .call()
    )

    persist_dry_raster_html = (
        persist_text.validate()
        .set_task_instance_id("persist_dry_raster_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="dry_mean_speed_raster_map.html",
            text=draw_dry_raster_map,
            **(params_dict.get("persist_dry_raster_html") or {}),
        )
        .call()
    )

    filter_wet_df = (
        filter_df.validate()
        .set_task_instance_id("filter_wet_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="season",
            op="equal",
            value="wet",
            df=add_season_labels,
            reset_index=False,
            **(params_dict.get("filter_wet_df") or {}),
        )
        .call()
    )

    generate_wet_speed_raster = (
        generate_ecograph_raster.validate()
        .set_task_instance_id("generate_wet_speed_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            step_length=500,
            dist_col="dist_meters",
            interpolation="mean",
            movement_covariate="speed",
            radius=2,
            cutoff=None,
            tortuosity_length=3,
            resolution=None,
            network_metric=None,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename=None,
            gdf=filter_wet_df,
            **(params_dict.get("generate_wet_speed_raster") or {}),
        )
        .call()
    )

    extract_wet_rasters = (
        retrieve_feature_gdf.validate()
        .set_task_instance_id("extract_wet_rasters")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            file_path=generate_wet_speed_raster,
            **(params_dict.get("extract_wet_rasters") or {}),
        )
        .call()
    )

    sort_wet_speed_features = (
        sort_values.validate()
        .set_task_instance_id("sort_wet_speed_features")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="value",
            na_position="last",
            ascending=True,
            df=extract_wet_rasters,
            **(params_dict.get("sort_wet_speed_features") or {}),
        )
        .call()
    )

    apply_classification_wet = (
        apply_classification.validate()
        .set_task_instance_id("apply_classification_wet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value",
            output_column_name="value_bins",
            classification_options={"scheme": "natural_breaks", "k": 6},
            label_options={"label_range": False, "label_decimals": 1},
            df=sort_wet_speed_features,
            **(params_dict.get("apply_classification_wet") or {}),
        )
        .call()
    )

    apply_wet_raster_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_wet_raster_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value_bins",
            output_column_name="speedraster_bins_colormap",
            colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
            df=apply_classification_wet,
            **(params_dict.get("apply_wet_raster_colormap") or {}),
        )
        .call()
    )

    format_wet_raster_labels = (
        map_values_with_unit.validate()
        .set_task_instance_id("format_wet_raster_labels")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="value_bins",
            output_column_name="bins_formatted",
            original_unit="km/h",
            new_unit="km/h",
            decimal_places=1,
            df=apply_wet_raster_colormap,
            **(params_dict.get("format_wet_raster_labels") or {}),
        )
        .call()
    )

    create_wet_speed_raster_layer = (
        create_geojson_layer.validate()
        .set_task_instance_id("create_wet_speed_raster_layer")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": False,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "speedraster_bins_colormap",
                "get_line_color": "speedraster_bins_colormap",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Mean Speed Raster (km/h)",
                "label_column": "bins_formatted",
                "color_column": "speedraster_bins_colormap",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=format_wet_raster_labels,
            **(params_dict.get("create_wet_speed_raster_layer") or {}),
        )
        .call()
    )

    combined_wet_speed_raster = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_wet_speed_raster")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=create_wet_speed_raster_layer,
            **(params_dict.get("combined_wet_speed_raster") or {}),
        )
        .call()
    )

    wet_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("wet_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=format_wet_raster_labels,
            pitch=0,
            bearing=0,
            **(params_dict.get("wet_view_state") or {}),
        )
        .call()
    )

    draw_wet_raster_map = (
        draw_map.validate()
        .set_task_instance_id("draw_wet_raster_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_wet_speed_raster,
            view_state=wet_view_state,
            **(params_dict.get("draw_wet_raster_map") or {}),
        )
        .call()
    )

    persist_wet_raster_html = (
        persist_text.validate()
        .set_task_instance_id("persist_wet_raster_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="wet_mean_speed_raster_map.html",
            text=draw_wet_raster_map,
            **(params_dict.get("persist_wet_raster_html") or {}),
        )
        .call()
    )

    generate_meshgrid = (
        create_meshgrid.validate()
        .set_task_instance_id("generate_meshgrid")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            aoi=annotate_day_night,
            auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
            crs="ESRI:53042",
            intersecting_only=False,
            **(params_dict.get("generate_meshgrid") or {}),
        )
        .call()
    )

    day_night_dominance = (
        get_day_night_dominance.validate()
        .set_task_instance_id("day_night_dominance")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            points_gdf=annotate_day_night,
            grid_gdf=generate_meshgrid,
            **(params_dict.get("day_night_dominance") or {}),
        )
        .call()
    )

    sort_dn_by_status = (
        sort_values.validate()
        .set_task_instance_id("sort_dn_by_status")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="is_night_majority",
            na_position="first",
            ascending=False,
            df=day_night_dominance,
            **(params_dict.get("sort_dn_by_status") or {}),
        )
        .call()
    )

    apply_dn_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_dn_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="is_night_majority",
            output_column_name="dn_colors",
            colormap=["#6495ed", "#00008b"],
            df=sort_dn_by_status,
            **(params_dict.get("apply_dn_colormap") or {}),
        )
        .call()
    )

    generate_dn_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_dn_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "dn_colors",
                "get_line_color": "dn_colors",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Time of day dominance",
                "label_column": "is_night_majority",
                "color_column": "dn_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=apply_dn_colormap,
            **(params_dict.get("generate_dn_layers") or {}),
        )
        .call()
    )

    combined_ldx_dn_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_dn_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_dn_layers,
            **(params_dict.get("combined_ldx_dn_layers") or {}),
        )
        .call()
    )

    dn_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("dn_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=apply_dn_colormap,
            pitch=0,
            bearing=0,
            **(params_dict.get("dn_view_state") or {}),
        )
        .call()
    )

    draw_dn_map = (
        draw_map.validate()
        .set_task_instance_id("draw_dn_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_dn_layers,
            view_state=dn_view_state,
            **(params_dict.get("draw_dn_map") or {}),
        )
        .call()
    )

    persist_dn_html = (
        persist_text.validate()
        .set_task_instance_id("persist_dn_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="time_of_day_dominance.html",
            text=draw_dn_map,
            **(params_dict.get("persist_dn_html") or {}),
        )
        .call()
    )

    get_night_fixes = (
        get_grid_night_fixes.validate()
        .set_task_instance_id("get_night_fixes")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            points_gdf=annotate_day_night,
            grid_gdf=generate_meshgrid,
            threshold=0.65,
            **(params_dict.get("get_night_fixes") or {}),
        )
        .call()
    )

    sort_night_by_status = (
        sort_values.validate()
        .set_task_instance_id("sort_night_by_status")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="night_activity",
            na_position="first",
            ascending=False,
            df=get_night_fixes,
            **(params_dict.get("sort_night_by_status") or {}),
        )
        .call()
    )

    apply_night_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_night_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="night_activity",
            output_column_name="night_colors",
            colormap=["#6495ed", "#00008b"],
            df=sort_night_by_status,
            **(params_dict.get("apply_night_colormap") or {}),
        )
        .call()
    )

    generate_night_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_night_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "night_colors",
                "get_line_color": "night_colors",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Night fixes (65%)",
                "label_column": "night_activity",
                "color_column": "night_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=apply_night_colormap,
            **(params_dict.get("generate_night_layers") or {}),
        )
        .call()
    )

    combined_ldx_night_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_night_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_night_layers,
            **(params_dict.get("combined_ldx_night_layers") or {}),
        )
        .call()
    )

    night_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("night_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=apply_night_colormap,
            pitch=0,
            bearing=0,
            **(params_dict.get("night_view_state") or {}),
        )
        .call()
    )

    draw_night_map = (
        draw_map.validate()
        .set_task_instance_id("draw_night_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_night_layers,
            view_state=night_view_state,
            **(params_dict.get("draw_night_map") or {}),
        )
        .call()
    )

    persist_night_html = (
        persist_text.validate()
        .set_task_instance_id("persist_night_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="night_fixes.html",
            text=draw_night_map,
            **(params_dict.get("persist_night_html") or {}),
        )
        .call()
    )

    protected_areas = (
        generate_protected_column.validate()
        .set_task_instance_id("protected_areas")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            trajs_gdf=add_season_labels,
            pa_gdf=filter_ldx_cols,
            column="type",
            **(params_dict.get("protected_areas") or {}),
        )
        .call()
    )

    filter_pa = (
        filter_df.validate()
        .set_task_instance_id("filter_pa")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=protected_areas,
            column_name="protection_status",
            op="equal",
            value="Protected",
            reset_index=False,
            **(params_dict.get("filter_pa") or {}),
        )
        .call()
    )

    generate_pa_etd = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("generate_pa_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            auto_scale_or_custom_cell_size={"auto_scale_or_customize": "Auto-scale"},
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
            nodata_value="nan",
            band_count=1,
            max_speed_factor=1.05,
            expansion_factor=1.3,
            trajectory_gdf=filter_pa,
            **(params_dict.get("generate_pa_etd") or {}),
        )
        .call()
    )

    persist_pa_etd_gdf = (
        persist_df.validate()
        .set_task_instance_id("persist_pa_etd_gdf")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="protected_areas_home_range_etd",
            df=generate_pa_etd,
            **(params_dict.get("persist_pa_etd_gdf") or {}),
        )
        .call()
    )

    apply_pa_etd_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_pa_etd_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            output_column_name="etd_percentile_colors",
            colormap="RdYlGn",
            df=generate_pa_etd,
            **(params_dict.get("apply_pa_etd_colormap") or {}),
        )
        .call()
    )

    generate_pa_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_pa_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "etd_percentile_colors",
                "get_line_color": "etd_percentile_colors",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Home Range Percentiles",
                "label_column": "percentile",
                "color_column": "etd_percentile_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=apply_pa_etd_colormap,
            **(params_dict.get("generate_pa_layers") or {}),
        )
        .call()
    )

    combined_ldx_pa_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_pa_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_pa_layers,
            **(params_dict.get("combined_ldx_pa_layers") or {}),
        )
        .call()
    )

    overall_pa_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("overall_pa_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=apply_pa_etd_colormap,
            pitch=0,
            bearing=0,
            **(params_dict.get("overall_pa_view_state") or {}),
        )
        .call()
    )

    draw_pa_map = (
        draw_map.validate()
        .set_task_instance_id("draw_pa_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_pa_layers,
            view_state=overall_pa_view_state,
            **(params_dict.get("draw_pa_map") or {}),
        )
        .call()
    )

    persist_pa_html = (
        persist_text.validate()
        .set_task_instance_id("persist_pa_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="protected_areas.html",
            text=draw_pa_map,
            **(params_dict.get("persist_pa_html") or {}),
        )
        .call()
    )

    filter_unprotected = (
        filter_df.validate()
        .set_task_instance_id("filter_unprotected")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=protected_areas,
            column_name="protection_status",
            op="equal",
            value="Unprotected",
            reset_index=False,
            **(params_dict.get("filter_unprotected") or {}),
        )
        .call()
    )

    generate_unprotected_etd = (
        calculate_elliptical_time_density.validate()
        .set_task_instance_id("generate_unprotected_etd")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            auto_scale_or_custom_cell_size={"auto_scale_or_customize": "Auto-scale"},
            crs="ESRI:53042",
            percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
            nodata_value="nan",
            band_count=1,
            max_speed_factor=1.05,
            expansion_factor=1.3,
            trajectory_gdf=filter_unprotected,
            **(params_dict.get("generate_unprotected_etd") or {}),
        )
        .call()
    )

    persist_unprotected_etd_gdf = (
        persist_df.validate()
        .set_task_instance_id("persist_unprotected_etd_gdf")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename="unprotected_areas_home_range_etd",
            df=generate_unprotected_etd,
            **(params_dict.get("persist_unprotected_etd_gdf") or {}),
        )
        .call()
    )

    apply_unprotected_etd_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("apply_unprotected_etd_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            input_column_name="percentile",
            output_column_name="etd_percentile_colors",
            colormap="RdYlGn",
            df=generate_unprotected_etd,
            **(params_dict.get("apply_unprotected_etd_colormap") or {}),
        )
        .call()
    )

    generate_unprotected_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_unprotected_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "etd_percentile_colors",
                "get_line_color": "etd_percentile_colors",
                "opacity": 0.55,
                "get_line_width": 1.55,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Home Range Percentiles",
                "label_column": "percentile",
                "color_column": "etd_percentile_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            geodataframe=apply_unprotected_etd_colormap,
            **(params_dict.get("generate_unprotected_layers") or {}),
        )
        .call()
    )

    combined_ldx_unprotected_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_unprotected_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            grouped_layers=generate_unprotected_layers,
            **(params_dict.get("combined_ldx_unprotected_layers") or {}),
        )
        .call()
    )

    overall_unprotected_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("overall_unprotected_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=apply_unprotected_etd_colormap,
            pitch=0,
            bearing=0,
            **(params_dict.get("overall_unprotected_view_state") or {}),
        )
        .call()
    )

    draw_unprotected_map = (
        draw_map.validate()
        .set_task_instance_id("draw_unprotected_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            geo_layers=combined_ldx_unprotected_layers,
            view_state=overall_unprotected_view_state,
            **(params_dict.get("draw_unprotected_map") or {}),
        )
        .call()
    )

    persist_unprotected_html = (
        persist_text.validate()
        .set_task_instance_id("persist_unprotected_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="unprotected_areas.html",
            text=draw_unprotected_map,
            **(params_dict.get("persist_unprotected_html") or {}),
        )
        .call()
    )

    plot_fix_protection_bar = (
        plot_fix_protection_status.validate()
        .set_task_instance_id("plot_fix_protection_bar")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            gdf=protected_areas, **(params_dict.get("plot_fix_protection_bar") or {})
        )
        .call()
    )

    persist_protection_bar = (
        persist_text.validate()
        .set_task_instance_id("persist_protection_bar")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filename="protection_status_bar.html",
            text=plot_fix_protection_bar,
            **(params_dict.get("persist_protection_bar") or {}),
        )
        .call()
    )

    split_traj_by_group = (
        split_groups.validate()
        .set_task_instance_id("split_traj_by_group")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=add_season_labels,
            groupers=groupers,
            **(params_dict.get("split_traj_by_group") or {}),
        )
        .call()
    )

    seasonal_home_range = (
        calculate_seasonal_home_range.validate()
        .set_task_instance_id("seasonal_home_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            groupby_cols=["season"],
            percentiles=[99.9],
            auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
            **(params_dict.get("seasonal_home_range") or {}),
        )
        .mapvalues(argnames=["gdf"], argvalues=split_traj_by_group)
    )

    convert_season_to_string = (
        convert_to_str.validate()
        .set_task_instance_id("convert_season_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["season"], **(params_dict.get("convert_season_to_string") or {})
        )
        .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
    )

    assign_season_df = (
        assign_season_colors.validate()
        .set_task_instance_id("assign_season_df")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(seasons_column="season", **(params_dict.get("assign_season_df") or {}))
        .mapvalues(argnames=["gdf"], argvalues=convert_season_to_string)
    )

    filter_season_cols = (
        filter_df_cols.validate()
        .set_task_instance_id("filter_season_cols")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            columns=["season_colors", "season", "geometry"],
            **(params_dict.get("filter_season_cols") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=assign_season_df)
    )

    generate_season_layers = (
        create_geojson_layer.validate()
        .set_task_instance_id("generate_season_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "filled": True,
                "stroked": True,
                "extruded": False,
                "wireframe": False,
                "get_fill_color": "season_colors",
                "get_line_color": "season_colors",
                "opacity": 0.55,
                "get_line_width": 0.35,
                "get_elevation": 0,
                "get_point_radius": 1,
                "line_width_units": "pixels",
                "line_width_scale": 1,
                "line_width_min_pixels": 1,
                "line_width_max_pixels": 5,
            },
            legend={
                "title": "Seasonal Home Range",
                "label_column": "season",
                "color_column": "season_colors",
                "sort": "ascending",
                "label_suffix": None,
            },
            **(params_dict.get("generate_season_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=filter_season_cols)
    )

    combined_ldx_seasonal_hr_layers = (
        combine_deckgl_map_layers.validate()
        .set_task_instance_id("combined_ldx_seasonal_hr_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
            **(params_dict.get("combined_ldx_seasonal_hr_layers") or {}),
        )
        .mapvalues(argnames=["grouped_layers"], argvalues=generate_season_layers)
    )

    season_view_state = (
        view_state_deck_gdf.validate()
        .set_task_instance_id("season_view_state")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(pitch=0, bearing=0, **(params_dict.get("season_view_state") or {}))
        .mapvalues(argnames=["gdf"], argvalues=filter_season_cols)
    )

    zip_seasonal_hr_with_viewstate = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_seasonal_hr_with_viewstate")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[combined_ldx_seasonal_hr_layers, season_view_state],
            **(params_dict.get("zip_seasonal_hr_with_viewstate") or {}),
        )
        .call()
    )

    draw_seasonal_home_range_map = (
        draw_map.validate()
        .set_task_instance_id("draw_seasonal_home_range_map")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=configure_base_maps,
            static=False,
            title=None,
            max_zoom=10,
            legend_style={"placement": "bottom-right"},
            **(params_dict.get("draw_seasonal_home_range_map") or {}),
        )
        .mapvalues(
            argnames=["geo_layers", "view_state"],
            argvalues=zip_seasonal_hr_with_viewstate,
        )
    )

    get_unique_subject_name = (
        dataframe_column_first_unique_str.validate()
        .set_task_instance_id("get_unique_subject_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            column_name="subject_name",
            **(params_dict.get("get_unique_subject_name") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_traj_by_group)
    )

    persist_seasonal_etd_gdf = (
        persist_df.validate()
        .set_task_instance_id("persist_seasonal_etd_gdf")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetype="geoparquet",
            filename=None,
            **(params_dict.get("persist_seasonal_etd_gdf") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=convert_season_to_string)
    )

    zip_seasonal_html_with_name = (
        zip_groupbykey.validate()
        .set_task_instance_id("zip_seasonal_html_with_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            sequences=[draw_seasonal_home_range_map, get_unique_subject_name],
            **(params_dict.get("zip_seasonal_html_with_name") or {}),
        )
        .call()
    )

    persist_seasonal_home_range_html = (
        persist_text.validate()
        .set_task_instance_id("persist_seasonal_home_range_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_seasonal_home_range_html") or {}),
        )
        .mapvalues(
            argnames=["text", "filename_suffix"], argvalues=zip_seasonal_html_with_name
        )
    )

    convert_movement_tracks = (
        html_to_png.validate()
        .set_task_instance_id("convert_movement_tracks")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_movement_tracks_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_movement_tracks") or {}),
        )
        .call()
    )

    convert_collared_points = (
        html_to_png.validate()
        .set_task_instance_id("convert_collared_points")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_collared_points_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_collared_points") or {}),
        )
        .call()
    )

    convert_subject_tracks = (
        html_to_png.validate()
        .set_task_instance_id("convert_subject_tracks")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_subject_tracks_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_subject_tracks") or {}),
        )
        .call()
    )

    convert_overall_homerange = (
        html_to_png.validate()
        .set_task_instance_id("convert_overall_homerange")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_homerange_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_overall_homerange") or {}),
        )
        .call()
    )

    convert_filtered_homerange = (
        html_to_png.validate()
        .set_task_instance_id("convert_filtered_homerange")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_filtered_hr_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_filtered_homerange") or {}),
        )
        .call()
    )

    convert_recursion_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_recursion_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_recursion_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_recursion_html") or {}),
        )
        .call()
    )

    convert_dry_raster_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_dry_raster_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_dry_raster_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_dry_raster_html") or {}),
        )
        .call()
    )

    convert_wet_raster_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_wet_raster_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_wet_raster_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_wet_raster_html") or {}),
        )
        .call()
    )

    convert_tdn_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_tdn_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_dn_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_tdn_html") or {}),
        )
        .call()
    )

    convert_night_fixes_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_night_fixes_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_night_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_night_fixes_html") or {}),
        )
        .call()
    )

    convert_pa_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_pa_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_pa_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_pa_html") or {}),
        )
        .call()
    )

    convert_upa_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_upa_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_unprotected_html,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_upa_html") or {}),
        )
        .call()
    )

    convert_bar_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_bar_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            html_path=persist_protection_bar,
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 100,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_bar_html") or {}),
        )
        .call()
    )

    convert_season_html = (
        html_to_png.validate()
        .set_task_instance_id("convert_season_html")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            config={
                "full_page": False,
                "device_scale_factor": 2.0,
                "wait_for_timeout": 40000,
                "max_concurrent_pages": 1,
            },
            **(params_dict.get("convert_season_html") or {}),
        )
        .mapvalues(argnames=["html_path"], argvalues=persist_seasonal_home_range_html)
    )

    download_cover_page = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_cover_page")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/20kwiv0nixcnyg0ciyvhn/general_ele_cover_page.docx?rlkey=xbde21rlzz2edxztkll8brw86&st=5ci1a6cq&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_cover_page") or {}),
        )
        .call()
    )

    download_general_template = (
        fetch_and_persist_file.validate()
        .set_task_instance_id("download_general_template")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            url="https://www.dropbox.com/scl/fi/nwe5k2qzly77a1bn0tb9w/general_elephant_grouper_page.docx?rlkey=nndhpfgysfueqadidu3au94rg&st=qlqjd5kl&dl=0",
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            overwrite_existing=False,
            unzip=False,
            retries=2,
            **(params_dict.get("download_general_template") or {}),
        )
        .call()
    )

    logo_path = (
        get_file_path.validate()
        .set_task_instance_id("logo_path")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("logo_path") or {}),
        )
        .call()
    )

    create_cover_tpl_context = (
        create_mapbook_ctx_cover.validate()
        .set_task_instance_id("create_cover_tpl_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            count=None,
            report_period=time_range,
            prepared_by="Ecoscope",
            org_logo_path=logo_path,
            **(params_dict.get("create_cover_tpl_context") or {}),
        )
        .call()
    )

    persist_cover_context = (
        create_context_page.validate()
        .set_task_instance_id("persist_cover_context")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            template_path=download_cover_page,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context=create_cover_tpl_context,
            filename="general_cover_page.docx",
            **(params_dict.get("persist_cover_context") or {}),
        )
        .call()
    )

    generate_template_report = (
        general_template_context.validate()
        .set_task_instance_id("generate_template_report")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            template_path=download_general_template,
            filename="general.docx",
            width=5.34,
            height=3.12,
            **(params_dict.get("generate_template_report") or {}),
        )
        .call()
    )

    merge_general_files = (
        merge_mapbook_files.validate()
        .set_task_instance_id("merge_general_files")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            cover_page_path=persist_cover_context,
            output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            context_page_items=[generate_template_report],
            filename=None,
            **(params_dict.get("merge_general_files") or {}),
        )
        .call()
    )

    general_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("general_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("general_dashboard") or {}),
        )
        .call()
    )

    return general_dashboard
