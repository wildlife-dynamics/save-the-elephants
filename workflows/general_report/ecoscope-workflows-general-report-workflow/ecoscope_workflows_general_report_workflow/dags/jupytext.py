# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details


# ruff: noqa: E402

# %% [markdown]
# # General Report
# TODO: top level description

# %% [markdown]
# ## Imports

import os

from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.io import set_gee_connection as set_gee_connection
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import filter_df as filter_df
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_core.tasks.transformation import (
    map_values_with_unit as map_values_with_unit,
)
from ecoscope_workflows_core.tasks.transformation import sort_values as sort_values
from ecoscope_workflows_ext_custom.tasks.io import html_to_png as html_to_png
from ecoscope_workflows_ext_custom.tasks.io import load_df as load_df
from ecoscope_workflows_ext_custom.tasks.results import (
    create_geojson_layer as create_geojson_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_path_layer as create_path_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import (
    create_scatterplot_layer as create_scatterplot_layer,
)
from ecoscope_workflows_ext_custom.tasks.results import draw_map as draw_map
from ecoscope_workflows_ext_custom.tasks.results import (
    set_base_maps_pydeck as set_base_maps_pydeck,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    filter_row_values as filter_row_values,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    calculate_elliptical_time_density as calculate_elliptical_time_density,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import (
    create_meshgrid as create_meshgrid,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import get_events as get_events
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_classification as apply_classification,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    classify_is_night as classify_is_night,
)
from ecoscope_workflows_ext_ste.tasks import (
    annotate_gdf_dict_with_geom_type as annotate_gdf_dict_with_geom_type,
)
from ecoscope_workflows_ext_ste.tasks import (
    assign_season_colors as assign_season_colors,
)
from ecoscope_workflows_ext_ste.tasks import (
    calculate_seasonal_home_range as calculate_seasonal_home_range,
)
from ecoscope_workflows_ext_ste.tasks import (
    combine_deckgl_map_layers as combine_deckgl_map_layers,
)
from ecoscope_workflows_ext_ste.tasks import convert_hex_to_rgba as convert_hex_to_rgba
from ecoscope_workflows_ext_ste.tasks import convert_to_str as convert_to_str
from ecoscope_workflows_ext_ste.tasks import create_column as create_column
from ecoscope_workflows_ext_ste.tasks import create_context_page as create_context_page
from ecoscope_workflows_ext_ste.tasks import (
    create_custom_text_layer as create_custom_text_layer,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_deckgl_layers_from_gdf_dict as create_deckgl_layers_from_gdf_dict,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_mapbook_ctx_cover as create_mapbook_ctx_cover,
)
from ecoscope_workflows_ext_ste.tasks import (
    create_seasonal_labels as create_seasonal_labels,
)
from ecoscope_workflows_ext_ste.tasks import (
    custom_determine_season_windows as custom_determine_season_windows,
)
from ecoscope_workflows_ext_ste.tasks import (
    custom_trajectory_segment_filter as custom_trajectory_segment_filter,
)
from ecoscope_workflows_ext_ste.tasks import (
    dataframe_column_first_unique_str as dataframe_column_first_unique_str,
)
from ecoscope_workflows_ext_ste.tasks import (
    determine_previous_period as determine_previous_period,
)
from ecoscope_workflows_ext_ste.tasks import (
    fetch_and_persist_file as fetch_and_persist_file,
)
from ecoscope_workflows_ext_ste.tasks import filter_df_cols as filter_df_cols
from ecoscope_workflows_ext_ste.tasks import filter_df_values as filter_df_values
from ecoscope_workflows_ext_ste.tasks import (
    general_template_context as general_template_context,
)
from ecoscope_workflows_ext_ste.tasks import (
    generate_ecograph_raster as generate_ecograph_raster,
)
from ecoscope_workflows_ext_ste.tasks import (
    generate_protected_column as generate_protected_column,
)
from ecoscope_workflows_ext_ste.tasks import (
    get_day_night_dominance as get_day_night_dominance,
)
from ecoscope_workflows_ext_ste.tasks import get_file_path as get_file_path
from ecoscope_workflows_ext_ste.tasks import (
    get_grid_night_fixes as get_grid_night_fixes,
)
from ecoscope_workflows_ext_ste.tasks import merge_mapbook_files as merge_mapbook_files
from ecoscope_workflows_ext_ste.tasks import merge_multiple_df as merge_multiple_df
from ecoscope_workflows_ext_ste.tasks import (
    modify_status_colors as modify_status_colors,
)
from ecoscope_workflows_ext_ste.tasks import (
    plot_fix_protection_status as plot_fix_protection_status,
)
from ecoscope_workflows_ext_ste.tasks import (
    retrieve_feature_gdf as retrieve_feature_gdf,
)
from ecoscope_workflows_ext_ste.tasks import split_gdf_by_column as split_gdf_by_column
from ecoscope_workflows_ext_ste.tasks import view_state_deck_gdf as view_state_deck_gdf
from ecoscope_workflows_ext_ste.tasks import zip_groupbykey as zip_groupbykey

# %% [markdown]
# ## Set workflow details

# %%
# parameters

workflow_details_params = dict(
    name=...,
    description=...,
    image_url=...,
)

# %%
# call the task


workflow_details = (
    set_workflow_details.set_task_instance_id("workflow_details")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**workflow_details_params)
    .call()
)


# %% [markdown]
# ## Define analysis time range

# %%
# parameters

time_range_params = dict(
    since=...,
    until=...,
    timezone=...,
    time_format=...,
)

# %%
# call the task


time_range = (
    set_time_range.set_task_instance_id("time_range")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**time_range_params)
    .call()
)


# %% [markdown]
# ## Configure grouping strategy

# %%
# parameters

groupers_params = dict()

# %%
# call the task


groupers = (
    set_groupers.set_task_instance_id("groupers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(groupers=[{"index_name": "subject_name"}], **groupers_params)
    .call()
)


# %% [markdown]
# ## Configure base map layers

# %%
# parameters

configure_base_maps_params = dict()

# %%
# call the task


configure_base_maps = (
    set_base_maps_pydeck.set_task_instance_id("configure_base_maps")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        base_maps=[
            {
                "url": "https://server.arcgisonline.com/arcgis/rest/services/Elevation/World_Hillshade/MapServer/tile/{z}/{y}/{x}",
                "opacity": 1,
                "max_zoom": 20,
            },
            {
                "url": "https://services.arcgisonline.com/ArcGIS/rest/services/Reference/World_Boundaries_and_Places_Alternate/MapServer/tile/{z}/{y}/{x}",
                "opacity": 0.25,
                "max_zoom": 20,
            },
        ],
        **configure_base_maps_params,
    )
    .call()
)


# %% [markdown]
# ## Set previous period range

# %%
# parameters

set_previous_period_params = dict(
    option=...,
)

# %%
# call the task


set_previous_period = (
    determine_previous_period.set_task_instance_id("set_previous_period")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(current_time_range=time_range, **set_previous_period_params)
    .call()
)


# %% [markdown]
# ## Connect to earth ranger

# %%
# parameters

er_client_name_params = dict(
    data_source=...,
)

# %%
# call the task


er_client_name = (
    set_er_connection.set_task_instance_id("er_client_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**er_client_name_params)
    .call()
)


# %% [markdown]
# ## Connect to earth engine

# %%
# parameters

gee_project_name_params = dict(
    data_source=...,
)

# %%
# call the task


gee_project_name = (
    set_gee_connection.set_task_instance_id("gee_project_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**gee_project_name_params)
    .call()
)


# %% [markdown]
# ##

# %%
# parameters

subject_group_var_params = dict(
    var=...,
)

# %%
# call the task


subject_group_var = (
    set_string_var.set_task_instance_id("subject_group_var")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**subject_group_var_params)
    .call()
)


# %% [markdown]
# ## Choose subject group to analyze

# %%
# parameters

subject_observations_params = dict()

# %%
# call the task


subject_observations = (
    get_subjectgroup_observations.set_task_instance_id("subject_observations")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        filter="clean",
        client=er_client_name,
        time_range=time_range,
        subject_group_name=subject_group_var,
        raise_on_empty=False,
        include_details=False,
        include_subjectsource_details=False,
        **subject_observations_params,
    )
    .call()
)


# %% [markdown]
# ## Load landDx database

# %%
# parameters

retrieve_ldx_db_params = dict(
    input_method=...,
)

# %%
# call the task


retrieve_ldx_db = (
    get_file_path.set_task_instance_id("retrieve_ldx_db")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"], **retrieve_ldx_db_params
    )
    .call()
)


# %% [markdown]
# ## Load ldx gpkg

# %%
# parameters

load_ldx_params = dict()

# %%
# call the task


load_ldx = (
    load_df.set_task_instance_id("load_ldx")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        file_path=retrieve_ldx_db,
        layer="landDx_polygons",
        deserialize_json=False,
        **load_ldx_params,
    )
    .call()
)


# %% [markdown]
# ## Filter loaded landDx by AOI

# %%
# parameters

filter_ldx_aoi_params = dict()

# %%
# call the task


filter_ldx_aoi = (
    filter_row_values.set_task_instance_id("filter_ldx_aoi")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=load_ldx,
        column="type",
        values=["Community Conservancy", "National Reserve", "National Park"],
        **filter_ldx_aoi_params,
    )
    .call()
)


# %% [markdown]
# ## Exclude unnecessary columns from ldx gdf

# %%
# parameters

filter_ldx_cols_params = dict()

# %%
# call the task


filter_ldx_cols = (
    filter_df_cols.set_task_instance_id("filter_ldx_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=filter_ldx_aoi,
        columns=["type", "name", "geometry"],
        **filter_ldx_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Create text layer

# %%
# parameters

create_ldx_text_layer_params = dict()

# %%
# call the task


create_ldx_text_layer = (
    create_custom_text_layer.set_task_instance_id("create_ldx_text_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        geodataframe=filter_ldx_cols,
        layer_style={
            "get_text": "name",
            "get_color": [20, 20, 20, 255],
            "get_size": 1000,
            "size_units": "meters",
            "size_min_pixels": 40,
            "size_max_pixels": 75,
            "size_scale": 1.25,
            "font_family": "Arial",
            "font_weight": "normal",
            "get_text_anchor": "middle",
            "get_alignment_baseline": "center",
            "billboard": True,
            "background_padding": [4, 8],
            "pickable": True,
            "auto_highlight": False,
        },
        use_centroid=True,
        legend=None,
        **create_ldx_text_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Split ldx gdf by type column

# %%
# parameters

split_ldx_by_type_params = dict()

# %%
# call the task


split_ldx_by_type = (
    split_gdf_by_column.set_task_instance_id("split_ldx_by_type")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=filter_ldx_cols, column="type", **split_ldx_by_type_params)
    .call()
)


# %% [markdown]
# ## Annotate gdf dict with geom type

# %%
# parameters

annotate_gdf_dict_params = dict()

# %%
# call the task


annotate_gdf_dict = (
    annotate_gdf_dict_with_geom_type.set_task_instance_id("annotate_gdf_dict")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf_dict=split_ldx_by_type, **annotate_gdf_dict_params)
    .call()
)


# %% [markdown]
# ## Create deckgl layers from split ldx gdf dict

# %%
# parameters

create_ldx_styled_layers_params = dict()

# %%
# call the task


create_ldx_styled_layers = (
    create_deckgl_layers_from_gdf_dict.set_task_instance_id("create_ldx_styled_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf_dict=annotate_gdf_dict,
        styles={
            "Community Conservancy": {
                "get_fill_color": [166, 182, 151],
                "get_line_color": [166, 182, 151],
                "opacity": 0.175,
                "stroked": True,
                "get_line_width": 2.25,
            },
            "National Reserve": {
                "get_fill_color": [136, 167, 142],
                "get_line_color": [136, 167, 142],
                "opacity": 0.175,
                "stroked": True,
                "get_line_width": 2.25,
            },
            "National Park": {
                "get_fill_color": [17, 86, 49],
                "get_line_color": [17, 86, 49],
                "opacity": 0.175,
                "stroked": True,
                "get_line_width": 2.25,
            },
        },
        legends={
            "title": "Land Use",
            "values": [
                {"label": "Community Conservancy", "color": "#a6b697"},
                {"label": "National Reserve", "color": "#88a78e"},
                {"label": "National Park", "color": "#115631"},
            ],
        },
        **create_ldx_styled_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Transform observations to relocations

# %%
# parameters

subject_reloc_params = dict()

# %%
# call the task


subject_reloc = (
    process_relocations.set_task_instance_id("subject_reloc")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        observations=subject_observations,
        relocs_columns=[
            "groupby_col",
            "fixtime",
            "junk_status",
            "geometry",
            "extra__subject__name",
            "extra__subject__hex",
            "extra__subject__sex",
            "extra__created_at",
            "extra__subject__subject_subtype",
        ],
        filter_point_coords=[
            {"x": 180.0, "y": 90.0},
            {"x": 0.0, "y": 0.0},
            {"x": 1.0, "y": 1.0},
        ],
        **subject_reloc_params,
    )
    .call()
)


# %% [markdown]
# ## Annotate relocations with day/night labels

# %%
# parameters

annotate_day_night_params = dict()

# %%
# call the task


annotate_day_night = (
    classify_is_night.set_task_instance_id("annotate_day_night")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(relocations=subject_reloc, **annotate_day_night_params)
    .call()
)


# %% [markdown]
# ## Trajectory Segment Filter

# %%
# parameters

custom_trajs_filter_params = dict(
    min_length_meters=...,
    max_length_meters=...,
    min_time_secs=...,
    max_time_secs=...,
    min_speed_kmhr=...,
    max_speed_kmhr=...,
)

# %%
# call the task


custom_trajs_filter = (
    custom_trajectory_segment_filter.set_task_instance_id("custom_trajs_filter")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(**custom_trajs_filter_params)
    .call()
)


# %% [markdown]
# ## Convert relocations to trajectories

# %%
# parameters

convert_to_trajectories_params = dict()

# %%
# call the task


convert_to_trajectories = (
    relocations_to_trajectory.set_task_instance_id("convert_to_trajectories")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        relocations=annotate_day_night,
        trajectory_segment_filter=custom_trajs_filter,
        **convert_to_trajectories_params,
    )
    .call()
)


# %% [markdown]
# ## Add temporal index to trajectories

# %%
# parameters

add_temporal_index_to_traj_params = dict()

# %%
# call the task


add_temporal_index_to_traj = (
    add_temporal_index.set_task_instance_id("add_temporal_index_to_traj")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=convert_to_trajectories,
        time_col="segment_start",
        groupers=groupers,
        cast_to_datetime=True,
        format="mixed",
        **add_temporal_index_to_traj_params,
    )
    .call()
)


# %% [markdown]
# ## Retrieve previous period observations

# %%
# parameters

previous_observations_params = dict()

# %%
# call the task


previous_observations = (
    get_subjectgroup_observations.set_task_instance_id("previous_observations")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        filter="clean",
        client=er_client_name,
        time_range=set_previous_period,
        subject_group_name=subject_group_var,
        raise_on_empty=False,
        include_details=False,
        include_subjectsource_details=False,
        **previous_observations_params,
    )
    .call()
)


# %% [markdown]
# ## Transform previous observations to relocations

# %%
# parameters

previous_relocs_params = dict()

# %%
# call the task


previous_relocs = (
    process_relocations.set_task_instance_id("previous_relocs")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        observations=previous_observations,
        relocs_columns=[
            "groupby_col",
            "fixtime",
            "junk_status",
            "geometry",
            "extra__subject__name",
            "extra__subject__hex",
            "extra__subject__sex",
            "extra__created_at",
            "extra__subject__subject_subtype",
        ],
        filter_point_coords=[
            {"x": 180.0, "y": 90.0},
            {"x": 0.0, "y": 0.0},
            {"x": 1.0, "y": 1.0},
        ],
        **previous_relocs_params,
    )
    .call()
)


# %% [markdown]
# ## Annotate previous relocations with day/night labels

# %%
# parameters

annotate_prev_day_night_params = dict()

# %%
# call the task


annotate_prev_day_night = (
    classify_is_night.set_task_instance_id("annotate_prev_day_night")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(relocations=previous_relocs, **annotate_prev_day_night_params)
    .call()
)


# %% [markdown]
# ## Convert  previous relocations to trajectories

# %%
# parameters

convert_prev_to_trajectories_params = dict()

# %%
# call the task


convert_prev_to_trajectories = (
    relocations_to_trajectory.set_task_instance_id("convert_prev_to_trajectories")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        relocations=annotate_prev_day_night,
        trajectory_segment_filter=custom_trajs_filter,
        **convert_prev_to_trajectories_params,
    )
    .call()
)


# %% [markdown]
# ## Add temporal index to previous trajectories

# %%
# parameters

add_temporal_prev_index_to_traj_params = dict()

# %%
# call the task


add_temporal_prev_index_to_traj = (
    add_temporal_index.set_task_instance_id("add_temporal_prev_index_to_traj")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=convert_prev_to_trajectories,
        time_col="segment_start",
        groupers=groupers,
        cast_to_datetime=True,
        format="mixed",
        **add_temporal_prev_index_to_traj_params,
    )
    .call()
)


# %% [markdown]
# ## Rename trajectory columns for previous period

# %%
# parameters

rename_prev_traj_cols_params = dict()

# %%
# call the task


rename_prev_traj_cols = (
    map_columns.set_task_instance_id("rename_prev_traj_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        raise_if_not_found=True,
        df=add_temporal_prev_index_to_traj,
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "extra__hex": "hex_color",
            "extra__is_night": "is_night",
            "extra__name": "subject_name",
            "extra__sex": "subject_sex",
            "extra__subject_subtype": "subject_subtype",
            "extra__created_at": "created_at",
        },
        **rename_prev_traj_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Classify trajectories by speed

# %%
# parameters

classify_trajectories_speed_bins_params = dict()

# %%
# call the task


classify_trajectories_speed_bins = (
    apply_classification.set_task_instance_id("classify_trajectories_speed_bins")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=add_temporal_index_to_traj,
        input_column_name="speed_kmhr",
        output_column_name="speed_bins",
        classification_options={"scheme": "equal_interval", "k": 6},
        label_options={
            "label_ranges": True,
            "label_decimals": 1,
            "label_suffix": " km/h",
        },
        **classify_trajectories_speed_bins_params,
    )
    .call()
)


# %% [markdown]
# ## Rename trajectory columns

# %%
# parameters

rename_traj_cols_params = dict()

# %%
# call the task


rename_traj_cols = (
    map_columns.set_task_instance_id("rename_traj_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        raise_if_not_found=True,
        df=classify_trajectories_speed_bins,
        drop_columns=[],
        retain_columns=[],
        rename_columns={
            "extra__hex": "hex_color",
            "extra__is_night": "is_night",
            "extra__name": "subject_name",
            "extra__sex": "subject_sex",
            "extra__subject_subtype": "subject_subtype",
            "extra__created_at": "created_at",
        },
        **rename_traj_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Persist previous period trajectories as geoparquet

# %%
# parameters

persist_prev_trajs_geoparquet_params = dict()

# %%
# call the task


persist_prev_trajs_geoparquet = (
    persist_df.set_task_instance_id("persist_prev_trajs_geoparquet")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=rename_prev_traj_cols,
        filetype="geoparquet",
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="previous_period_trajectories",
        **persist_prev_trajs_geoparquet_params,
    )
    .call()
)


# %% [markdown]
# ## Persist relocations as geoparquet

# %%
# parameters

persist_relocs_geoparquet_params = dict()

# %%
# call the task


persist_relocs_geoparquet = (
    persist_df.set_task_instance_id("persist_relocs_geoparquet")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=annotate_day_night,
        filetype="geoparquet",
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="relocations",
        **persist_relocs_geoparquet_params,
    )
    .call()
)


# %% [markdown]
# ## Persist previous period relocations as geoparquet

# %%
# parameters

persist_prev_relocs_geoparquet_params = dict()

# %%
# call the task


persist_prev_relocs_geoparquet = (
    persist_df.set_task_instance_id("persist_prev_relocs_geoparquet")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=previous_relocs,
        filetype="geoparquet",
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="previous_period_relocations",
        **persist_prev_relocs_geoparquet_params,
    )
    .call()
)


# %% [markdown]
# ## Create column for current duration status

# %%
# parameters

create_current_duration_column_params = dict()

# %%
# call the task


create_current_duration_column = (
    create_column.set_task_instance_id("create_current_duration_column")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=rename_traj_cols,
        col_name="duration_status",
        value="Current tracks",
        **create_current_duration_column_params,
    )
    .call()
)


# %% [markdown]
# ## Create column for previous duration status

# %%
# parameters

create_previous_duration_column_params = dict()

# %%
# call the task


create_previous_duration_column = (
    create_column.set_task_instance_id("create_previous_duration_column")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=rename_prev_traj_cols,
        col_name="duration_status",
        value="Previous tracks",
        **create_previous_duration_column_params,
    )
    .call()
)


# %% [markdown]
# ## Merge current - previous trajectories

# %%
# parameters

merge_current_prev_trajs_params = dict()

# %%
# call the task


merge_current_prev_trajs = (
    merge_multiple_df.set_task_instance_id("merge_current_prev_trajs")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        list_df=[create_current_duration_column, create_previous_duration_column],
        ignore_index=True,
        sort=False,
        **merge_current_prev_trajs_params,
    )
    .call()
)


# %% [markdown]
# ## Assign duration colors

# %%
# parameters

assign_duration_colors_params = dict()

# %%
# call the task


assign_duration_colors = (
    modify_status_colors.set_task_instance_id("assign_duration_colors")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        grouper_value="overall",
        gdf=merge_current_prev_trajs,
        **assign_duration_colors_params,
    )
    .call()
)


# %% [markdown]
# ## Sort trajectories by duration status

# %%
# parameters

sort_trajs_by_status_params = dict()

# %%
# call the task


sort_trajs_by_status = (
    sort_values.set_task_instance_id("sort_trajs_by_status")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="duration_status",
        na_position="first",
        ascending=False,
        df=assign_duration_colors,
        **sort_trajs_by_status_params,
    )
    .call()
)


# %% [markdown]
# ## Exclude unnecessary columns from movement gdf

# %%
# parameters

filter_movement_cols_params = dict()

# %%
# call the task


filter_movement_cols = (
    filter_df_cols.set_task_instance_id("filter_movement_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        columns=["duration_status", "duration_status_colors", "is_night", "geometry"],
        df=sort_trajs_by_status,
        **filter_movement_cols_params,
    )
    .call()
)


# %% [markdown]
# ## Create movement track layers

# %%
# parameters

generate_track_layers_params = dict()

# %%
# call the task


generate_track_layers = (
    create_path_layer.set_task_instance_id("generate_track_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_color": "duration_status_colors",
            "get_width": 2.85,
            "width_scale": 1,
            "width_min_pixels": 2,
            "width_max_pixels": 8,
            "width_units": "pixels",
            "cap_rounded": True,
            "joint_rounded": True,
            "billboard": False,
            "opacity": 0.55,
            "stroked": True,
        },
        legend={
            "title": "Movement Tracks",
            "label_column": "duration_status",
            "color_column": "duration_status_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=filter_movement_cols,
        **generate_track_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and movement track layers

# %%
# parameters

combined_ldx_movement_layers_params = dict()

# %%
# call the task


combined_ldx_movement_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_movement_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_track_layers,
        **combined_ldx_movement_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

movement_view_state_params = dict()

# %%
# call the task


movement_view_state = (
    view_state_deck_gdf.set_task_instance_id("movement_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=filter_movement_cols, pitch=0, bearing=0, **movement_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw movement tracks map

# %%
# parameters

draw_movement_tracks_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_movement_tracks = (
    draw_map.set_task_instance_id("draw_movement_tracks")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        view_state=movement_view_state,
        geo_layers=combined_ldx_movement_layers,
        **draw_movement_tracks_params,
    )
    .call()
)


# %% [markdown]
# ## Persist movement tracks html

# %%
# parameters

persist_movement_tracks_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_movement_tracks_html = (
    persist_text.set_task_instance_id("persist_movement_tracks_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="movement_tracks.html",
        text=draw_movement_tracks,
        **persist_movement_tracks_html_params,
    )
    .call()
)


# %% [markdown]
# ## Retrieve all events

# %%
# parameters

get_events_data_params = dict(
    event_types=...,
)

# %%
# call the task


get_events_data = (
    get_events.set_task_instance_id("get_events_data")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        client=er_client_name,
        time_range=time_range,
        event_columns=[
            "id",
            "time",
            "event_type",
            "event_category",
            "reported_by",
            "serial_number",
            "geometry",
            "created_at",
            "event_details",
        ],
        raise_on_empty=False,
        include_details=True,
        include_updates=False,
        include_related_events=False,
        include_null_geometry=False,
        include_display_values=False,
        **get_events_data_params,
    )
    .call()
)


# %% [markdown]
# ## Generate elephant collaring locations

# %%
# parameters

generate_collaring_layers_params = dict()

# %%
# call the task


generate_collaring_layers = (
    create_scatterplot_layer.set_task_instance_id("generate_collaring_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_fill_color": [85, 107, 47],
            "get_line_color": [0, 0, 0, 200],
            "get_line_width": 0.55,
            "get_radius": 3.55,
            "opacity": 0.75,
            "stroked": True,
        },
        legend={
            "title": "Legend",
            "values": [{"label": "Elephant sightings", "color": "#556b2f"}],
        },
        geodataframe=get_events_data,
        **generate_collaring_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and collared point layers

# %%
# parameters

combined_ldx_collar_points_params = dict()

# %%
# call the task


combined_ldx_collar_points = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_collar_points")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_collaring_layers,
        **combined_ldx_collar_points_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

collaring_view_state_params = dict()

# %%
# call the task


collaring_view_state = (
    view_state_deck_gdf.set_task_instance_id("collaring_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=get_events_data, pitch=0, bearing=0, **collaring_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw collaring points map

# %%
# parameters

draw_collared_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_collared_map = (
    draw_map.set_task_instance_id("draw_collared_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        view_state=collaring_view_state,
        geo_layers=combined_ldx_collar_points,
        **draw_collared_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist collared points html

# %%
# parameters

persist_collared_points_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_collared_points_html = (
    persist_text.set_task_instance_id("persist_collared_points_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="collared_points.html",
        text=draw_collared_map,
        **persist_collared_points_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert subject hex colors  to rgba colors

# %%
# parameters

convert_subject_hex_rgba_params = dict()

# %%
# call the task


convert_subject_hex_rgba = (
    convert_hex_to_rgba.set_task_instance_id("convert_subject_hex_rgba")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=rename_traj_cols,
        col="hex_color",
        new_col="rgba_colors",
        **convert_subject_hex_rgba_params,
    )
    .call()
)


# %% [markdown]
# ## Create subject track layers

# %%
# parameters

generate_strack_layers_params = dict()

# %%
# call the task


generate_strack_layers = (
    create_path_layer.set_task_instance_id("generate_strack_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "get_color": "rgba_colors",
            "get_width": 2.85,
            "width_scale": 1,
            "width_min_pixels": 2,
            "width_max_pixels": 8,
            "width_units": "pixels",
            "cap_rounded": True,
            "joint_rounded": True,
            "billboard": False,
            "opacity": 0.55,
            "stroked": True,
        },
        legend={
            "title": "Movement Tracks",
            "label_column": "subject_name",
            "color_column": "rgba_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=convert_subject_hex_rgba,
        **generate_strack_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and subject track layers

# %%
# parameters

combined_ldx_subject_layers_params = dict()

# %%
# call the task


combined_ldx_subject_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_subject_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_strack_layers,
        **combined_ldx_subject_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

tracks_view_state_params = dict()

# %%
# call the task


tracks_view_state = (
    view_state_deck_gdf.set_task_instance_id("tracks_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=convert_subject_hex_rgba, pitch=0, bearing=0, **tracks_view_state_params
    )
    .call()
)


# %% [markdown]
# ## Draw subject tracks map

# %%
# parameters

draw_subject_tracks_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_subject_tracks = (
    draw_map.set_task_instance_id("draw_subject_tracks")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        view_state=tracks_view_state,
        geo_layers=combined_ldx_subject_layers,
        **draw_subject_tracks_params,
    )
    .call()
)


# %% [markdown]
# ## Persist subject tracks html

# %%
# parameters

persist_subject_tracks_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_subject_tracks_html = (
    persist_text.set_task_instance_id("persist_subject_tracks_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="subject_tracks.html",
        text=draw_subject_tracks,
        **persist_subject_tracks_html_params,
    )
    .call()
)


# %% [markdown]
# ## Generate etd

# %%
# parameters

generate_etd_params = dict()

# %%
# call the task


generate_etd = (
    calculate_elliptical_time_density.set_task_instance_id("generate_etd")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        auto_scale_or_custom_cell_size={"auto_scale_or_customize": "Auto-scale"},
        crs="ESRI:53042",
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        trajectory_gdf=rename_traj_cols,
        **generate_etd_params,
    )
    .call()
)


# %% [markdown]
# ## Persist etd gdf

# %%
# parameters

persist_etd_gdf_params = dict()

# %%
# call the task


persist_etd_gdf = (
    persist_df.set_task_instance_id("persist_etd_gdf")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="geoparquet",
        filename="home_range_etd",
        df=generate_etd,
        **persist_etd_gdf_params,
    )
    .call()
)


# %% [markdown]
# ## Determine seasonal windows

# %%
# parameters

determine_seasonal_windows_params = dict()

# %%
# call the task


determine_seasonal_windows = (
    custom_determine_season_windows.set_task_instance_id("determine_seasonal_windows")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        client=gee_project_name,
        time_range=time_range,
        roi=generate_etd,
        **determine_seasonal_windows_params,
    )
    .call()
)


# %% [markdown]
# ## Persist seasonal windows csv

# %%
# parameters

persist_ndvi_values_params = dict()

# %%
# call the task


persist_ndvi_values = (
    persist_df.set_task_instance_id("persist_ndvi_values")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="csv",
        filename="seasonal_windows",
        df=determine_seasonal_windows,
        **persist_ndvi_values_params,
    )
    .call()
)


# %% [markdown]
# ## Create seasonal labels

# %%
# parameters

add_season_labels_params = dict()

# %%
# call the task


add_season_labels = (
    create_seasonal_labels.set_task_instance_id("add_season_labels")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        seasons_df=determine_seasonal_windows,
        trajectories=rename_traj_cols,
        **add_season_labels_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to home range percentiles

# %%
# parameters

apply_etd_colormap_params = dict()

# %%
# call the task


apply_etd_colormap = (
    apply_color_map.set_task_instance_id("apply_etd_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="percentile",
        output_column_name="etd_percentile_colors",
        colormap="RdYlGn",
        df=generate_etd,
        **apply_etd_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Create home range layers

# %%
# parameters

generate_home_range_layers_params = dict()

# %%
# call the task


generate_home_range_layers = (
    create_geojson_layer.set_task_instance_id("generate_home_range_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "etd_percentile_colors",
            "get_line_color": "etd_percentile_colors",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Home Range Percentiles",
            "label_column": "percentile",
            "color_column": "etd_percentile_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=apply_etd_colormap,
        **generate_home_range_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and home range layers

# %%
# parameters

combined_ldx_home_range_layers_params = dict()

# %%
# call the task


combined_ldx_home_range_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_home_range_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_home_range_layers,
        **combined_ldx_home_range_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

overall_hr_view_state_params = dict()

# %%
# call the task


overall_hr_view_state = (
    view_state_deck_gdf.set_task_instance_id("overall_hr_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=apply_etd_colormap, pitch=0, bearing=0, **overall_hr_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw home range map

# %%
# parameters

draw_home_range_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_home_range_map = (
    draw_map.set_task_instance_id("draw_home_range_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_home_range_layers,
        view_state=overall_hr_view_state,
        **draw_home_range_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist homerange html

# %%
# parameters

persist_homerange_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_homerange_html = (
    persist_text.set_task_instance_id("persist_homerange_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="overall_homerange.html",
        text=draw_home_range_map,
        **persist_homerange_html_params,
    )
    .call()
)


# %% [markdown]
# ## Filter etd to 99th percentile

# %%
# parameters

filter_percentile_params = dict()

# %%
# call the task


filter_percentile = (
    filter_df_values.set_task_instance_id("filter_percentile")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="percentile",
        op="ge",
        value=99.0,
        df=generate_etd,
        reset_index=True,
        **filter_percentile_params,
    )
    .call()
)


# %% [markdown]
# ## Create 99 home range layers

# %%
# parameters

custom_home_range_layers_params = dict()

# %%
# call the task


custom_home_range_layers = (
    create_geojson_layer.set_task_instance_id("custom_home_range_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": [210, 105, 30],
            "get_line_color": [210, 105, 30],
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Home Range Percentiles",
            "values": [{"label": "99th percentile", "color": "#d2691e"}],
        },
        geodataframe=filter_percentile,
        **custom_home_range_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and filtered home range layers

# %%
# parameters

combined_ldx_filtered_hr_layers_params = dict()

# %%
# call the task


combined_ldx_filtered_hr_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_filtered_hr_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=custom_home_range_layers,
        **combined_ldx_filtered_hr_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

filtered_hr_view_state_params = dict()

# %%
# call the task


filtered_hr_view_state = (
    view_state_deck_gdf.set_task_instance_id("filtered_hr_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=filter_percentile, pitch=0, bearing=0, **filtered_hr_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw filtered home range map

# %%
# parameters

draw_filtered_hr_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_filtered_hr_map = (
    draw_map.set_task_instance_id("draw_filtered_hr_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_filtered_hr_layers,
        view_state=filtered_hr_view_state,
        **draw_filtered_hr_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist homerange html

# %%
# parameters

persist_filtered_hr_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_filtered_hr_html = (
    persist_text.set_task_instance_id("persist_filtered_hr_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="filtered_homerange.html",
        text=draw_filtered_hr_map,
        **persist_filtered_hr_html_params,
    )
    .call()
)


# %% [markdown]
# ## Generate recursion events raster

# %%
# parameters

generate_recursion_raster_params = dict()

# %%
# call the task


generate_recursion_raster = (
    generate_ecograph_raster.set_task_instance_id("generate_recursion_raster")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        step_length=2000,
        dist_col="dist_meters",
        interpolation="mean",
        movement_covariate=None,
        radius=2,
        cutoff=None,
        tortuosity_length=3,
        resolution=None,
        network_metric="weight",
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="weighted_raster",
        gdf=add_season_labels,
        **generate_recursion_raster_params,
    )
    .call()
)


# %% [markdown]
# ## Extract features from raster

# %%
# parameters

extract_raster_params = dict()

# %%
# call the task


extract_raster = (
    retrieve_feature_gdf.set_task_instance_id("extract_raster")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(file_path=generate_recursion_raster, **extract_raster_params)
    .call()
)


# %% [markdown]
# ## Sort features by value

# %%
# parameters

sort_recursion_features_params = dict()

# %%
# call the task


sort_recursion_features = (
    sort_values.set_task_instance_id("sort_recursion_features")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="value",
        na_position="last",
        ascending=True,
        df=extract_raster,
        **sort_recursion_features_params,
    )
    .call()
)


# %% [markdown]
# ## Classify recursion gdfs

# %%
# parameters

classify_recursion_features_params = dict()

# %%
# call the task


classify_recursion_features = (
    apply_classification.set_task_instance_id("classify_recursion_features")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value",
        output_column_name="bins",
        classification_options={"scheme": "natural_breaks", "k": 6},
        label_options={"label_ranges": False, "label_decimals": 1},
        df=sort_recursion_features,
        **classify_recursion_features_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to recursion bins

# %%
# parameters

apply_recursion_colormap_params = dict()

# %%
# call the task


apply_recursion_colormap = (
    apply_color_map.set_task_instance_id("apply_recursion_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="bins",
        output_column_name="recursion_bins_colors",
        colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
        df=classify_recursion_features,
        **apply_recursion_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Create recursion events layers

# %%
# parameters

generate_recursion_layers_params = dict()

# %%
# call the task


generate_recursion_layers = (
    create_geojson_layer.set_task_instance_id("generate_recursion_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "recursion_bins_colors",
            "get_line_color": "recursion_bins_colors",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Recursion events",
            "label_column": "bins",
            "color_column": "recursion_bins_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=apply_recursion_colormap,
        **generate_recursion_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and recursion events layers

# %%
# parameters

combined_ldx_recursion_layers_params = dict()

# %%
# call the task


combined_ldx_recursion_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_recursion_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_recursion_layers,
        **combined_ldx_recursion_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

recursion_view_state_params = dict()

# %%
# call the task


recursion_view_state = (
    view_state_deck_gdf.set_task_instance_id("recursion_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=apply_recursion_colormap, pitch=0, bearing=0, **recursion_view_state_params
    )
    .call()
)


# %% [markdown]
# ## Draw recursion events map

# %%
# parameters

draw_recursion_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_recursion_map = (
    draw_map.set_task_instance_id("draw_recursion_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_recursion_layers,
        view_state=recursion_view_state,
        **draw_recursion_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist recursion events html

# %%
# parameters

persist_recursion_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_recursion_html = (
    persist_text.set_task_instance_id("persist_recursion_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="recursion_events.html",
        text=draw_recursion_map,
        **persist_recursion_html_params,
    )
    .call()
)


# %% [markdown]
# ## Persist trajectories as geoparquet

# %%
# parameters

persist_trajs_geoparquet_params = dict()

# %%
# call the task


persist_trajs_geoparquet = (
    persist_df.set_task_instance_id("persist_trajs_geoparquet")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=add_season_labels,
        filetype="geoparquet",
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="trajectories",
        **persist_trajs_geoparquet_params,
    )
    .call()
)


# %% [markdown]
# ## Retrieve dry trajs only

# %%
# parameters

filter_dry_df_params = dict()

# %%
# call the task


filter_dry_df = (
    filter_df.set_task_instance_id("filter_dry_df")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="season",
        op="equal",
        value="dry",
        df=add_season_labels,
        reset_index=False,
        **filter_dry_df_params,
    )
    .call()
)


# %% [markdown]
# ## Generate dry season mean speed raster

# %%
# parameters

generate_dry_speed_raster_params = dict()

# %%
# call the task


generate_dry_speed_raster = (
    generate_ecograph_raster.set_task_instance_id("generate_dry_speed_raster")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        step_length=500,
        dist_col="dist_meters",
        interpolation="mean",
        movement_covariate="speed",
        radius=2,
        cutoff=None,
        tortuosity_length=3,
        resolution=None,
        network_metric=None,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename=None,
        gdf=filter_dry_df,
        **generate_dry_speed_raster_params,
    )
    .call()
)


# %% [markdown]
# ## Extract features from dry season mean speed raster

# %%
# parameters

extract_dry_rasters_params = dict()

# %%
# call the task


extract_dry_rasters = (
    retrieve_feature_gdf.set_task_instance_id("extract_dry_rasters")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(file_path=generate_dry_speed_raster, **extract_dry_rasters_params)
    .call()
)


# %% [markdown]
# ## Sort dry speed features by value

# %%
# parameters

sort_dry_speed_features_params = dict()

# %%
# call the task


sort_dry_speed_features = (
    sort_values.set_task_instance_id("sort_dry_speed_features")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="value",
        na_position="last",
        ascending=True,
        df=extract_dry_rasters,
        **sort_dry_speed_features_params,
    )
    .call()
)


# %% [markdown]
# ## Apply classification to dry speed raster features

# %%
# parameters

apply_classification_dry_params = dict()

# %%
# call the task


apply_classification_dry = (
    apply_classification.set_task_instance_id("apply_classification_dry")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value",
        output_column_name="value_bins",
        classification_options={"scheme": "natural_breaks", "k": 6},
        label_options={"label_range": False, "label_decimals": 1},
        df=sort_dry_speed_features,
        **apply_classification_dry_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to dry speed raster bins

# %%
# parameters

apply_dry_raster_colormap_params = dict()

# %%
# call the task


apply_dry_raster_colormap = (
    apply_color_map.set_task_instance_id("apply_dry_raster_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value_bins",
        output_column_name="speedraster_bins_colormap",
        colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
        df=apply_classification_dry,
        **apply_dry_raster_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Format dry speed raster bin labels

# %%
# parameters

format_dry_raster_labels_params = dict()

# %%
# call the task


format_dry_raster_labels = (
    map_values_with_unit.set_task_instance_id("format_dry_raster_labels")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value_bins",
        output_column_name="bins_formatted",
        original_unit="km/h",
        new_unit="km/h",
        decimal_places=1,
        df=apply_dry_raster_colormap,
        **format_dry_raster_labels_params,
    )
    .call()
)


# %% [markdown]
# ## Create dry mean speed raster layer

# %%
# parameters

create_dry_speed_raster_layer_params = dict()

# %%
# call the task


create_dry_speed_raster_layer = (
    create_geojson_layer.set_task_instance_id("create_dry_speed_raster_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": False,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "speedraster_bins_colormap",
            "get_line_color": "speedraster_bins_colormap",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Mean Speed Raster (km/h)",
            "label_column": "bins_formatted",
            "color_column": "speedraster_bins_colormap",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=format_dry_raster_labels,
        **create_dry_speed_raster_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and dry mean speed raster layer

# %%
# parameters

combined_dry_speed_raster_params = dict()

# %%
# call the task


combined_dry_speed_raster = (
    combine_deckgl_map_layers.set_task_instance_id("combined_dry_speed_raster")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=create_dry_speed_raster_layer,
        **combined_dry_speed_raster_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

dry_view_state_params = dict()

# %%
# call the task


dry_view_state = (
    view_state_deck_gdf.set_task_instance_id("dry_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=format_dry_raster_labels, pitch=0, bearing=0, **dry_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw dry season mean speed raster map

# %%
# parameters

draw_dry_raster_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_dry_raster_map = (
    draw_map.set_task_instance_id("draw_dry_raster_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_dry_speed_raster,
        view_state=dry_view_state,
        **draw_dry_raster_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist dry season mean speed raster map html

# %%
# parameters

persist_dry_raster_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_dry_raster_html = (
    persist_text.set_task_instance_id("persist_dry_raster_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="dry_mean_speed_raster_map.html",
        text=draw_dry_raster_map,
        **persist_dry_raster_html_params,
    )
    .call()
)


# %% [markdown]
# ## Retrieve wet trajs only

# %%
# parameters

filter_wet_df_params = dict()

# %%
# call the task


filter_wet_df = (
    filter_df.set_task_instance_id("filter_wet_df")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="season",
        op="equal",
        value="wet",
        df=add_season_labels,
        reset_index=False,
        **filter_wet_df_params,
    )
    .call()
)


# %% [markdown]
# ## Generate wet season mean speed raster

# %%
# parameters

generate_wet_speed_raster_params = dict()

# %%
# call the task


generate_wet_speed_raster = (
    generate_ecograph_raster.set_task_instance_id("generate_wet_speed_raster")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        step_length=500,
        dist_col="dist_meters",
        interpolation="mean",
        movement_covariate="speed",
        radius=2,
        cutoff=None,
        tortuosity_length=3,
        resolution=None,
        network_metric=None,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename=None,
        gdf=filter_wet_df,
        **generate_wet_speed_raster_params,
    )
    .call()
)


# %% [markdown]
# ## Extract features from wet season mean speed raster

# %%
# parameters

extract_wet_rasters_params = dict()

# %%
# call the task


extract_wet_rasters = (
    retrieve_feature_gdf.set_task_instance_id("extract_wet_rasters")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(file_path=generate_wet_speed_raster, **extract_wet_rasters_params)
    .call()
)


# %% [markdown]
# ## Sort wet speed features by value

# %%
# parameters

sort_wet_speed_features_params = dict()

# %%
# call the task


sort_wet_speed_features = (
    sort_values.set_task_instance_id("sort_wet_speed_features")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="value",
        na_position="last",
        ascending=True,
        df=extract_wet_rasters,
        **sort_wet_speed_features_params,
    )
    .call()
)


# %% [markdown]
# ## Apply classification to wet speed raster features

# %%
# parameters

apply_classification_wet_params = dict()

# %%
# call the task


apply_classification_wet = (
    apply_classification.set_task_instance_id("apply_classification_wet")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value",
        output_column_name="value_bins",
        classification_options={"scheme": "natural_breaks", "k": 6},
        label_options={"label_range": False, "label_decimals": 1},
        df=sort_wet_speed_features,
        **apply_classification_wet_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to wet speed raster bins

# %%
# parameters

apply_wet_raster_colormap_params = dict()

# %%
# call the task


apply_wet_raster_colormap = (
    apply_color_map.set_task_instance_id("apply_wet_raster_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value_bins",
        output_column_name="speedraster_bins_colormap",
        colormap=["#1a9850", "#91cf60", "#d9ef8b", "#fee08b", "#fc8d59", "#d73027"],
        df=apply_classification_wet,
        **apply_wet_raster_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Format wet speed raster bin labels

# %%
# parameters

format_wet_raster_labels_params = dict()

# %%
# call the task


format_wet_raster_labels = (
    map_values_with_unit.set_task_instance_id("format_wet_raster_labels")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="value_bins",
        output_column_name="bins_formatted",
        original_unit="km/h",
        new_unit="km/h",
        decimal_places=1,
        df=apply_wet_raster_colormap,
        **format_wet_raster_labels_params,
    )
    .call()
)


# %% [markdown]
# ## Create wet mean speed raster layer

# %%
# parameters

create_wet_speed_raster_layer_params = dict()

# %%
# call the task


create_wet_speed_raster_layer = (
    create_geojson_layer.set_task_instance_id("create_wet_speed_raster_layer")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": False,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "speedraster_bins_colormap",
            "get_line_color": "speedraster_bins_colormap",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Mean Speed Raster (km/h)",
            "label_column": "bins_formatted",
            "color_column": "speedraster_bins_colormap",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=format_wet_raster_labels,
        **create_wet_speed_raster_layer_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and wet mean speed raster layer

# %%
# parameters

combined_wet_speed_raster_params = dict()

# %%
# call the task


combined_wet_speed_raster = (
    combine_deckgl_map_layers.set_task_instance_id("combined_wet_speed_raster")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=create_wet_speed_raster_layer,
        **combined_wet_speed_raster_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

wet_view_state_params = dict()

# %%
# call the task


wet_view_state = (
    view_state_deck_gdf.set_task_instance_id("wet_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=format_wet_raster_labels, pitch=0, bearing=0, **wet_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw wet season mean speed raster map

# %%
# parameters

draw_wet_raster_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_wet_raster_map = (
    draw_map.set_task_instance_id("draw_wet_raster_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_wet_speed_raster,
        view_state=wet_view_state,
        **draw_wet_raster_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist wet season mean speed raster map html

# %%
# parameters

persist_wet_raster_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_wet_raster_html = (
    persist_text.set_task_instance_id("persist_wet_raster_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="wet_mean_speed_raster_map.html",
        text=draw_wet_raster_map,
        **persist_wet_raster_html_params,
    )
    .call()
)


# %% [markdown]
# ## Generate meshgrid

# %%
# parameters

generate_meshgrid_params = dict()

# %%
# call the task


generate_meshgrid = (
    create_meshgrid.set_task_instance_id("generate_meshgrid")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        aoi=annotate_day_night,
        auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
        crs="ESRI:53042",
        intersecting_only=False,
        **generate_meshgrid_params,
    )
    .call()
)


# %% [markdown]
# ## Generate day night time dominance

# %%
# parameters

day_night_dominance_params = dict()

# %%
# call the task


day_night_dominance = (
    get_day_night_dominance.set_task_instance_id("day_night_dominance")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        points_gdf=annotate_day_night,
        grid_gdf=generate_meshgrid,
        **day_night_dominance_params,
    )
    .call()
)


# %% [markdown]
# ## Sort day night dominance by duration status

# %%
# parameters

sort_dn_by_status_params = dict()

# %%
# call the task


sort_dn_by_status = (
    sort_values.set_task_instance_id("sort_dn_by_status")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="is_night_majority",
        na_position="first",
        ascending=False,
        df=day_night_dominance,
        **sort_dn_by_status_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to day night

# %%
# parameters

apply_dn_colormap_params = dict()

# %%
# call the task


apply_dn_colormap = (
    apply_color_map.set_task_instance_id("apply_dn_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="is_night_majority",
        output_column_name="dn_colors",
        colormap=["#6495ed", "#00008b"],
        df=sort_dn_by_status,
        **apply_dn_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Create day night dominance layers

# %%
# parameters

generate_dn_layers_params = dict()

# %%
# call the task


generate_dn_layers = (
    create_geojson_layer.set_task_instance_id("generate_dn_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "dn_colors",
            "get_line_color": "dn_colors",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Time of day dominance",
            "label_column": "is_night_majority",
            "color_column": "dn_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=apply_dn_colormap,
        **generate_dn_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and time of day layers

# %%
# parameters

combined_ldx_dn_layers_params = dict()

# %%
# call the task


combined_ldx_dn_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_dn_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_dn_layers,
        **combined_ldx_dn_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

dn_view_state_params = dict()

# %%
# call the task


dn_view_state = (
    view_state_deck_gdf.set_task_instance_id("dn_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=apply_dn_colormap, pitch=0, bearing=0, **dn_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw time of day dominance map

# %%
# parameters

draw_dn_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_dn_map = (
    draw_map.set_task_instance_id("draw_dn_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_dn_layers,
        view_state=dn_view_state,
        **draw_dn_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist time of day dominance html

# %%
# parameters

persist_dn_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_dn_html = (
    persist_text.set_task_instance_id("persist_dn_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="time_of_day_dominance.html",
        text=draw_dn_map,
        **persist_dn_html_params,
    )
    .call()
)


# %% [markdown]
# ## Get grid night fixes

# %%
# parameters

get_night_fixes_params = dict()

# %%
# call the task


get_night_fixes = (
    get_grid_night_fixes.set_task_instance_id("get_night_fixes")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        points_gdf=annotate_day_night,
        grid_gdf=generate_meshgrid,
        threshold=0.65,
        **get_night_fixes_params,
    )
    .call()
)


# %% [markdown]
# ## Sort night fixes by duration status

# %%
# parameters

sort_night_by_status_params = dict()

# %%
# call the task


sort_night_by_status = (
    sort_values.set_task_instance_id("sort_night_by_status")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        column_name="night_activity",
        na_position="first",
        ascending=False,
        df=get_night_fixes,
        **sort_night_by_status_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to night fixes

# %%
# parameters

apply_night_colormap_params = dict()

# %%
# call the task


apply_night_colormap = (
    apply_color_map.set_task_instance_id("apply_night_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="night_activity",
        output_column_name="night_colors",
        colormap=["#6495ed", "#00008b"],
        df=sort_night_by_status,
        **apply_night_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Create night fixes layers

# %%
# parameters

generate_night_layers_params = dict()

# %%
# call the task


generate_night_layers = (
    create_geojson_layer.set_task_instance_id("generate_night_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "night_colors",
            "get_line_color": "night_colors",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Night fixes (65%)",
            "label_column": "night_activity",
            "color_column": "night_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=apply_night_colormap,
        **generate_night_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and night fixes layers

# %%
# parameters

combined_ldx_night_layers_params = dict()

# %%
# call the task


combined_ldx_night_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_night_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_night_layers,
        **combined_ldx_night_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

night_view_state_params = dict()

# %%
# call the task


night_view_state = (
    view_state_deck_gdf.set_task_instance_id("night_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=apply_night_colormap, pitch=0, bearing=0, **night_view_state_params)
    .call()
)


# %% [markdown]
# ## Draw night fixes map

# %%
# parameters

draw_night_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_night_map = (
    draw_map.set_task_instance_id("draw_night_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_night_layers,
        view_state=night_view_state,
        **draw_night_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist night fixes html

# %%
# parameters

persist_night_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_night_html = (
    persist_text.set_task_instance_id("persist_night_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="night_fixes.html",
        text=draw_night_map,
        **persist_night_html_params,
    )
    .call()
)


# %% [markdown]
# ## Determine protected areas

# %%
# parameters

protected_areas_params = dict()

# %%
# call the task


protected_areas = (
    generate_protected_column.set_task_instance_id("protected_areas")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        trajs_gdf=add_season_labels,
        pa_gdf=filter_ldx_cols,
        column="type",
        **protected_areas_params,
    )
    .call()
)


# %% [markdown]
# ## Filter protected areas

# %%
# parameters

filter_pa_params = dict()

# %%
# call the task


filter_pa = (
    filter_df.set_task_instance_id("filter_pa")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=protected_areas,
        column_name="protection_status",
        op="equal",
        value="Protected",
        reset_index=False,
        **filter_pa_params,
    )
    .call()
)


# %% [markdown]
# ## Generate protected areas etd

# %%
# parameters

generate_pa_etd_params = dict()

# %%
# call the task


generate_pa_etd = (
    calculate_elliptical_time_density.set_task_instance_id("generate_pa_etd")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        auto_scale_or_custom_cell_size={"auto_scale_or_customize": "Auto-scale"},
        crs="ESRI:53042",
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        trajectory_gdf=filter_pa,
        **generate_pa_etd_params,
    )
    .call()
)


# %% [markdown]
# ## Persist protected areas etd gdf

# %%
# parameters

persist_pa_etd_gdf_params = dict()

# %%
# call the task


persist_pa_etd_gdf = (
    persist_df.set_task_instance_id("persist_pa_etd_gdf")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="geoparquet",
        filename="protected_areas_home_range_etd",
        df=generate_pa_etd,
        **persist_pa_etd_gdf_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to protected areas percentiles

# %%
# parameters

apply_pa_etd_colormap_params = dict()

# %%
# call the task


apply_pa_etd_colormap = (
    apply_color_map.set_task_instance_id("apply_pa_etd_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="percentile",
        output_column_name="etd_percentile_colors",
        colormap="RdYlGn",
        df=generate_pa_etd,
        **apply_pa_etd_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Create protected areas layers

# %%
# parameters

generate_pa_layers_params = dict()

# %%
# call the task


generate_pa_layers = (
    create_geojson_layer.set_task_instance_id("generate_pa_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "etd_percentile_colors",
            "get_line_color": "etd_percentile_colors",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Home Range Percentiles",
            "label_column": "percentile",
            "color_column": "etd_percentile_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=apply_pa_etd_colormap,
        **generate_pa_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and protected areas layers

# %%
# parameters

combined_ldx_pa_layers_params = dict()

# %%
# call the task


combined_ldx_pa_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_pa_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_pa_layers,
        **combined_ldx_pa_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

overall_pa_view_state_params = dict()

# %%
# call the task


overall_pa_view_state = (
    view_state_deck_gdf.set_task_instance_id("overall_pa_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=apply_pa_etd_colormap, pitch=0, bearing=0, **overall_pa_view_state_params
    )
    .call()
)


# %% [markdown]
# ## Draw protected areas map

# %%
# parameters

draw_pa_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_pa_map = (
    draw_map.set_task_instance_id("draw_pa_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_pa_layers,
        view_state=overall_pa_view_state,
        **draw_pa_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist protected areas html

# %%
# parameters

persist_pa_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_pa_html = (
    persist_text.set_task_instance_id("persist_pa_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="protected_areas.html",
        text=draw_pa_map,
        **persist_pa_html_params,
    )
    .call()
)


# %% [markdown]
# ## Filter unprotected areas

# %%
# parameters

filter_unprotected_params = dict()

# %%
# call the task


filter_unprotected = (
    filter_df.set_task_instance_id("filter_unprotected")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        df=protected_areas,
        column_name="protection_status",
        op="equal",
        value="Unprotected",
        reset_index=False,
        **filter_unprotected_params,
    )
    .call()
)


# %% [markdown]
# ## Generate unprotected areas etd

# %%
# parameters

generate_unprotected_etd_params = dict()

# %%
# call the task


generate_unprotected_etd = (
    calculate_elliptical_time_density.set_task_instance_id("generate_unprotected_etd")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        auto_scale_or_custom_cell_size={"auto_scale_or_customize": "Auto-scale"},
        crs="ESRI:53042",
        percentiles=[50.0, 60.0, 70.0, 80.0, 90.0, 95.0, 99.9],
        nodata_value="nan",
        band_count=1,
        max_speed_factor=1.05,
        expansion_factor=1.3,
        trajectory_gdf=filter_unprotected,
        **generate_unprotected_etd_params,
    )
    .call()
)


# %% [markdown]
# ## Persist unprotected areas etd gdf

# %%
# parameters

persist_unprotected_etd_gdf_params = dict()

# %%
# call the task


persist_unprotected_etd_gdf = (
    persist_df.set_task_instance_id("persist_unprotected_etd_gdf")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="geoparquet",
        filename="unprotected_areas_home_range_etd",
        df=generate_unprotected_etd,
        **persist_unprotected_etd_gdf_params,
    )
    .call()
)


# %% [markdown]
# ## Apply colormap to unprotected areas percentiles

# %%
# parameters

apply_unprotected_etd_colormap_params = dict()

# %%
# call the task


apply_unprotected_etd_colormap = (
    apply_color_map.set_task_instance_id("apply_unprotected_etd_colormap")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        input_column_name="percentile",
        output_column_name="etd_percentile_colors",
        colormap="RdYlGn",
        df=generate_unprotected_etd,
        **apply_unprotected_etd_colormap_params,
    )
    .call()
)


# %% [markdown]
# ## Create unprotected areas layers

# %%
# parameters

generate_unprotected_layers_params = dict()

# %%
# call the task


generate_unprotected_layers = (
    create_geojson_layer.set_task_instance_id("generate_unprotected_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "etd_percentile_colors",
            "get_line_color": "etd_percentile_colors",
            "opacity": 0.55,
            "get_line_width": 1.55,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Home Range Percentiles",
            "label_column": "percentile",
            "color_column": "etd_percentile_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        geodataframe=apply_unprotected_etd_colormap,
        **generate_unprotected_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Combine ldx layers and unprotected areas layers

# %%
# parameters

combined_ldx_unprotected_layers_params = dict()

# %%
# call the task


combined_ldx_unprotected_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_unprotected_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        grouped_layers=generate_unprotected_layers,
        **combined_ldx_unprotected_layers_params,
    )
    .call()
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

overall_unprotected_view_state_params = dict()

# %%
# call the task


overall_unprotected_view_state = (
    view_state_deck_gdf.set_task_instance_id("overall_unprotected_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        gdf=apply_unprotected_etd_colormap,
        pitch=0,
        bearing=0,
        **overall_unprotected_view_state_params,
    )
    .call()
)


# %% [markdown]
# ## Draw unprotected areas map

# %%
# parameters

draw_unprotected_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_unprotected_map = (
    draw_map.set_task_instance_id("draw_unprotected_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        geo_layers=combined_ldx_unprotected_layers,
        view_state=overall_unprotected_view_state,
        **draw_unprotected_map_params,
    )
    .call()
)


# %% [markdown]
# ## Persist unprotected areas html

# %%
# parameters

persist_unprotected_html_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_unprotected_html = (
    persist_text.set_task_instance_id("persist_unprotected_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="unprotected_areas.html",
        text=draw_unprotected_map,
        **persist_unprotected_html_params,
    )
    .call()
)


# %% [markdown]
# ## Plot protection status bar chart

# %%
# parameters

plot_fix_protection_bar_params = dict()

# %%
# call the task


plot_fix_protection_bar = (
    plot_fix_protection_status.set_task_instance_id("plot_fix_protection_bar")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(gdf=protected_areas, **plot_fix_protection_bar_params)
    .call()
)


# %% [markdown]
# ## Persist protection status bar chart

# %%
# parameters

persist_protection_bar_params = dict(
    filename_suffix=...,
)

# %%
# call the task


persist_protection_bar = (
    persist_text.set_task_instance_id("persist_protection_bar")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filename="protection_status_bar.html",
        text=plot_fix_protection_bar,
        **persist_protection_bar_params,
    )
    .call()
)


# %% [markdown]
# ## Split trajectories by group

# %%
# parameters

split_traj_by_group_params = dict()

# %%
# call the task


split_traj_by_group = (
    split_groups.set_task_instance_id("split_traj_by_group")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(df=add_season_labels, groupers=groupers, **split_traj_by_group_params)
    .call()
)


# %% [markdown]
# ## Calculate seasonal home range

# %%
# parameters

seasonal_home_range_params = dict()

# %%
# call the task


seasonal_home_range = (
    calculate_seasonal_home_range.set_task_instance_id("seasonal_home_range")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        groupby_cols=["season"],
        percentiles=[99.9],
        auto_scale_or_custom_cell_size={"auto_scale_or_custom": "Auto-scale"},
        **seasonal_home_range_params,
    )
    .mapvalues(argnames=["gdf"], argvalues=split_traj_by_group)
)


# %% [markdown]
# ## Convert season column to string

# %%
# parameters

convert_season_to_string_params = dict()

# %%
# call the task


convert_season_to_string = (
    convert_to_str.set_task_instance_id("convert_season_to_string")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(columns=["season"], **convert_season_to_string_params)
    .mapvalues(argnames=["df"], argvalues=seasonal_home_range)
)


# %% [markdown]
# ## Assign season colors to dataframe

# %%
# parameters

assign_season_df_params = dict()

# %%
# call the task


assign_season_df = (
    assign_season_colors.set_task_instance_id("assign_season_df")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(seasons_column="season", **assign_season_df_params)
    .mapvalues(argnames=["gdf"], argvalues=convert_season_to_string)
)


# %% [markdown]
# ## Exclude unnecessary columns from mean speed raster gdf

# %%
# parameters

filter_season_cols_params = dict()

# %%
# call the task


filter_season_cols = (
    filter_df_cols.set_task_instance_id("filter_season_cols")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        columns=["season_colors", "season", "geometry"], **filter_season_cols_params
    )
    .mapvalues(argnames=["df"], argvalues=assign_season_df)
)


# %% [markdown]
# ## Create seasonal home range layers

# %%
# parameters

generate_season_layers_params = dict()

# %%
# call the task


generate_season_layers = (
    create_geojson_layer.set_task_instance_id("generate_season_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        layer_style={
            "filled": True,
            "stroked": True,
            "extruded": False,
            "wireframe": False,
            "get_fill_color": "season_colors",
            "get_line_color": "season_colors",
            "opacity": 0.55,
            "get_line_width": 0.35,
            "get_elevation": 0,
            "get_point_radius": 1,
            "line_width_units": "pixels",
            "line_width_scale": 1,
            "line_width_min_pixels": 1,
            "line_width_max_pixels": 5,
        },
        legend={
            "title": "Seasonal Home Range",
            "label_column": "season",
            "color_column": "season_colors",
            "sort": "ascending",
            "label_suffix": None,
        },
        **generate_season_layers_params,
    )
    .mapvalues(argnames=["geodataframe"], argvalues=filter_season_cols)
)


# %% [markdown]
# ## Combine ldx layers and seasonal home range layers

# %%
# parameters

combined_ldx_seasonal_hr_layers_params = dict()

# %%
# call the task


combined_ldx_seasonal_hr_layers = (
    combine_deckgl_map_layers.set_task_instance_id("combined_ldx_seasonal_hr_layers")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        static_layers=[create_ldx_styled_layers, create_ldx_text_layer],
        **combined_ldx_seasonal_hr_layers_params,
    )
    .mapvalues(argnames=["grouped_layers"], argvalues=generate_season_layers)
)


# %% [markdown]
# ## Zoom to gdf extent

# %%
# parameters

season_view_state_params = dict()

# %%
# call the task


season_view_state = (
    view_state_deck_gdf.set_task_instance_id("season_view_state")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(pitch=0, bearing=0, **season_view_state_params)
    .mapvalues(argnames=["gdf"], argvalues=filter_season_cols)
)


# %% [markdown]
# ## Combine seasonal home range layers with view state

# %%
# parameters

zip_seasonal_hr_with_viewstate_params = dict()

# %%
# call the task


zip_seasonal_hr_with_viewstate = (
    zip_groupbykey.set_task_instance_id("zip_seasonal_hr_with_viewstate")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[combined_ldx_seasonal_hr_layers, season_view_state],
        **zip_seasonal_hr_with_viewstate_params,
    )
    .call()
)


# %% [markdown]
# ## Draw seasonal home range map

# %%
# parameters

draw_seasonal_home_range_map_params = dict(
    widget_id=...,
)

# %%
# call the task


draw_seasonal_home_range_map = (
    draw_map.set_task_instance_id("draw_seasonal_home_range_map")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        tile_layers=configure_base_maps,
        static=False,
        title=None,
        max_zoom=10,
        legend_style={"placement": "bottom-right"},
        **draw_seasonal_home_range_map_params,
    )
    .mapvalues(
        argnames=["geo_layers", "view_state"], argvalues=zip_seasonal_hr_with_viewstate
    )
)


# %% [markdown]
# ## Get unique name for seasonal home range

# %%
# parameters

get_unique_subject_name_params = dict()

# %%
# call the task


get_unique_subject_name = (
    dataframe_column_first_unique_str.set_task_instance_id("get_unique_subject_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(column_name="subject_name", **get_unique_subject_name_params)
    .mapvalues(argnames=["df"], argvalues=split_traj_by_group)
)


# %% [markdown]
# ## Persist seasonal etd gdf

# %%
# parameters

persist_seasonal_etd_gdf_params = dict()

# %%
# call the task


persist_seasonal_etd_gdf = (
    persist_df.set_task_instance_id("persist_seasonal_etd_gdf")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetype="geoparquet",
        filename=None,
        **persist_seasonal_etd_gdf_params,
    )
    .mapvalues(argnames=["df"], argvalues=convert_season_to_string)
)


# %% [markdown]
# ## Zip seasonal html with subject names

# %%
# parameters

zip_seasonal_html_with_name_params = dict()

# %%
# call the task


zip_seasonal_html_with_name = (
    zip_groupbykey.set_task_instance_id("zip_seasonal_html_with_name")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        sequences=[draw_seasonal_home_range_map, get_unique_subject_name],
        **zip_seasonal_html_with_name_params,
    )
    .call()
)


# %% [markdown]
# ## Persist seasonal home range html

# %%
# parameters

persist_seasonal_home_range_html_params = dict(
    filename=...,
)

# %%
# call the task


persist_seasonal_home_range_html = (
    persist_text.set_task_instance_id("persist_seasonal_home_range_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_seasonal_home_range_html_params,
    )
    .mapvalues(
        argnames=["text", "filename_suffix"], argvalues=zip_seasonal_html_with_name
    )
)


# %% [markdown]
# ## Convert movement tracks html to png

# %%
# parameters

convert_movement_tracks_params = dict()

# %%
# call the task


convert_movement_tracks = (
    html_to_png.set_task_instance_id("convert_movement_tracks")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_movement_tracks_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_movement_tracks_params,
    )
    .call()
)


# %% [markdown]
# ## Convert collared points html to png

# %%
# parameters

convert_collared_points_params = dict()

# %%
# call the task


convert_collared_points = (
    html_to_png.set_task_instance_id("convert_collared_points")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_collared_points_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_collared_points_params,
    )
    .call()
)


# %% [markdown]
# ## Convert subject tracks html to png

# %%
# parameters

convert_subject_tracks_params = dict()

# %%
# call the task


convert_subject_tracks = (
    html_to_png.set_task_instance_id("convert_subject_tracks")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_subject_tracks_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_subject_tracks_params,
    )
    .call()
)


# %% [markdown]
# ## Convert overall homerange html to png

# %%
# parameters

convert_overall_homerange_params = dict()

# %%
# call the task


convert_overall_homerange = (
    html_to_png.set_task_instance_id("convert_overall_homerange")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_homerange_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_overall_homerange_params,
    )
    .call()
)


# %% [markdown]
# ## Convert filtered homerange html to png

# %%
# parameters

convert_filtered_homerange_params = dict()

# %%
# call the task


convert_filtered_homerange = (
    html_to_png.set_task_instance_id("convert_filtered_homerange")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_filtered_hr_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_filtered_homerange_params,
    )
    .call()
)


# %% [markdown]
# ## Convert recursion html to png

# %%
# parameters

convert_recursion_html_params = dict()

# %%
# call the task


convert_recursion_html = (
    html_to_png.set_task_instance_id("convert_recursion_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_recursion_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_recursion_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert dry raster html to png

# %%
# parameters

convert_dry_raster_html_params = dict()

# %%
# call the task


convert_dry_raster_html = (
    html_to_png.set_task_instance_id("convert_dry_raster_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_dry_raster_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_dry_raster_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert wet raster html to png

# %%
# parameters

convert_wet_raster_html_params = dict()

# %%
# call the task


convert_wet_raster_html = (
    html_to_png.set_task_instance_id("convert_wet_raster_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_wet_raster_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_wet_raster_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert time of day dominance html to png

# %%
# parameters

convert_tdn_html_params = dict()

# %%
# call the task


convert_tdn_html = (
    html_to_png.set_task_instance_id("convert_tdn_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_dn_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_tdn_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert night fixes html to png

# %%
# parameters

convert_night_fixes_html_params = dict()

# %%
# call the task


convert_night_fixes_html = (
    html_to_png.set_task_instance_id("convert_night_fixes_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_night_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_night_fixes_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert protected areas html to png

# %%
# parameters

convert_pa_html_params = dict()

# %%
# call the task


convert_pa_html = (
    html_to_png.set_task_instance_id("convert_pa_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_pa_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_pa_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert unprotected areas html to png

# %%
# parameters

convert_upa_html_params = dict()

# %%
# call the task


convert_upa_html = (
    html_to_png.set_task_instance_id("convert_upa_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_unprotected_html,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_upa_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert protected-unprotected bar html to png

# %%
# parameters

convert_bar_html_params = dict()

# %%
# call the task


convert_bar_html = (
    html_to_png.set_task_instance_id("convert_bar_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        html_path=persist_protection_bar,
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 100,
            "max_concurrent_pages": 1,
        },
        **convert_bar_html_params,
    )
    .call()
)


# %% [markdown]
# ## Convert season html to png

# %%
# parameters

convert_season_html_params = dict()

# %%
# call the task


convert_season_html = (
    html_to_png.set_task_instance_id("convert_season_html")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        config={
            "full_page": False,
            "device_scale_factor": 2.0,
            "wait_for_timeout": 40000,
            "max_concurrent_pages": 1,
        },
        **convert_season_html_params,
    )
    .mapvalues(argnames=["html_path"], argvalues=persist_seasonal_home_range_html)
)


# %% [markdown]
# ## Download general report cover page

# %%
# parameters

download_cover_page_params = dict()

# %%
# call the task


download_cover_page = (
    fetch_and_persist_file.set_task_instance_id("download_cover_page")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/20kwiv0nixcnyg0ciyvhn/general_ele_cover_page.docx?rlkey=xbde21rlzz2edxztkll8brw86&st=5ci1a6cq&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        unzip=False,
        retries=2,
        **download_cover_page_params,
    )
    .call()
)


# %% [markdown]
# ## Download general elephant template report

# %%
# parameters

download_general_template_params = dict()

# %%
# call the task


download_general_template = (
    fetch_and_persist_file.set_task_instance_id("download_general_template")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        url="https://www.dropbox.com/scl/fi/nwe5k2qzly77a1bn0tb9w/general_elephant_grouper_page.docx?rlkey=nndhpfgysfueqadidu3au94rg&st=qlqjd5kl&dl=0",
        output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        overwrite_existing=False,
        unzip=False,
        retries=2,
        **download_general_template_params,
    )
    .call()
)


# %% [markdown]
# ## Report logo

# %%
# parameters

logo_path_params = dict(
    input_method=...,
)

# %%
# call the task


logo_path = (
    get_file_path.set_task_instance_id("logo_path")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(output_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"], **logo_path_params)
    .call()
)


# %% [markdown]
# ## Create cover page template context

# %%
# parameters

create_cover_tpl_context_params = dict()

# %%
# call the task


create_cover_tpl_context = (
    create_mapbook_ctx_cover.set_task_instance_id("create_cover_tpl_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        count=None,
        report_period=time_range,
        prepared_by="Ecoscope",
        org_logo_path=logo_path,
        **create_cover_tpl_context_params,
    )
    .call()
)


# %% [markdown]
# ## Persist cover page

# %%
# parameters

persist_cover_context_params = dict()

# %%
# call the task


persist_cover_context = (
    create_context_page.set_task_instance_id("persist_cover_context")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        template_path=download_cover_page,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        context=create_cover_tpl_context,
        filename="general_cover_page.docx",
        **persist_cover_context_params,
    )
    .call()
)


# %% [markdown]
# ## Create general template context

# %%
# parameters

generate_template_report_params = dict()

# %%
# call the task


generate_template_report = (
    general_template_context.set_task_instance_id("generate_template_report")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        template_path=download_general_template,
        filename="general.docx",
        width=5.34,
        height=3.12,
        **generate_template_report_params,
    )
    .call()
)


# %% [markdown]
# ## Merge docx files

# %%
# parameters

merge_general_files_params = dict()

# %%
# call the task


merge_general_files = (
    merge_mapbook_files.set_task_instance_id("merge_general_files")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        cover_page_path=persist_cover_context,
        output_dir=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        context_page_items=[generate_template_report],
        filename=None,
        **merge_general_files_params,
    )
    .call()
)


# %% [markdown]
# ## General report dashboard

# %%
# parameters

general_dashboard_params = dict(
    warning=...,
)

# %%
# call the task


general_dashboard = (
    gather_dashboard.set_task_instance_id("general_dashboard")
    .handle_errors()
    .with_tracing()
    .skipif(
        conditions=[
            any_is_empty_df,
            any_dependency_skipped,
        ],
        unpack_depth=1,
    )
    .partial(
        details=workflow_details,
        widgets=[],
        time_range=time_range,
        groupers=groupers,
        **general_dashboard_params,
    )
    .call()
)
